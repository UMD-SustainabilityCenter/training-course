{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae187963",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "auto-generated-toc"
    ]
   },
   "source": [
    "## Table of Contents\n",
    "- [Preprocessing 1D **Microphone Data** for Conversion to 2D Image Format](#Preprocessing-1D-%2A%2AMicrophone-Data%2A%2A-for-Conversion-to-2D-Image-Format)\n",
    "  - [Background:](#Background%3A)\n",
    "  - [Read raw microphone data](#Read-raw-microphone-data)\n",
    "  - [Removing noise through STFT and FFT filtering](#Removing-noise-through-STFT-and-FFT-filtering)\n",
    "- [üè† Home](../../../../../welcomePage.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be962c-b3fb-4d7d-a529-42a73dcc9659",
   "metadata": {},
   "source": [
    "# Preprocessing 1D **Microphone Data** for Conversion to 2D Image Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da41bd9-9f23-4f0f-b54e-fe24efa6b8e4",
   "metadata": {},
   "source": [
    "## Background:\n",
    "\n",
    "**A method to filter raw microphone signal is demonstrated in this section.**\n",
    "In this work, the airborne acoustic emission signal of the process zone is captured by a free-field standard microphone (G.R.A.S.\r",
    "46AE 1/2‚Ä≤‚Ä≤ CCP), with a frequency range of 3.15 Hz‚Äì20 kHz (¬±2 dB) and a sensitivity of 50 mV/Pa. The microphone is installed inside the build chamber at an angle of about 45‚ó¶ facing the center of the building zone. The distance from the laser scanning area to the\n",
    "microphone is approximately 200 mm. The microphone is connected via a NI-9218 data acquisition card. The overall monitoring system is controlled by the LabView software, which provides convenience for adjusting the acquisition parameters.\n",
    "\n",
    "<img src=\"./figures/2.png\" alt=\"Drawing\" style=\"width: 400px;\" title=\" Experimental setup and a schematic representation of the SLM system\"/>\n",
    "\n",
    "*Experimental setup and a schematic representation of the SLM system (The microphone sensor in red circle)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc3ec8-4548-4535-8bad-4cd67bc8a8b7",
   "metadata": {},
   "source": [
    "### Process detail\n",
    "\n",
    "1. **Short-time Fourier transform (STFT)**. The raw acoustic signal was first analyzed by the STFT to extract the time frequency information, aiming to remove the environment noise. The STFT operation was carried out in the Origin software using the signals processing tool, and the default hanning window function with a default length of 256 was applied to deal with the acoustic signals.\n",
    "2. **High pass FFT filtering**. A 4 kHz high pass FFT filtering was employed to remove the low frequency environmental noise.\n",
    "3. **Noise removal**. The acoustic signal of the processing and non-processing can be easily distinguished after the FFT filtering. Consequently, the interference of background noise was removed and the acoustic emission signal during the laser melting process was extracted for the subsequent analysis.\n",
    "5. The raw 1D acoustic or photodiode signal with a length of $M^2$ is extracted through a running window, where $M^2$ is the size of the\n",
    "converted image. The running window is selected by considering that it can not only increase the number of training samples but also help train the quality monitoring models with local sensor information. Then, the running window with $M^2$ samples in the time domain fulfills the pixels of the image by sequence. Detailly, consecutive 1D signal samples with a length of $M$ fulfill rows of the converted image in sequence. For the $L(i), i = 1, 2, ‚Ä¶, M^2$ sample in the 1D signal, its coordinate corresponding to the pixel in the image pixel is $P(j,k)$.\n",
    "\n",
    "$P(j, k)=\\operatorname{round}\\left\\{\\frac{L((j-1) \\times M+k)-\\min (L)}{\\max (L)-\\min (L)} \\times 255\\right\\},(j, k=1,2, \\ldots, M)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a7bbf4-7b69-44f4-ac39-91aa77a9026b",
   "metadata": {},
   "source": [
    "### The information users need to provide:\n",
    "1. The raw photodiode data local directory path.\n",
    "2. The converted image data save folder local directory path.\n",
    "3. The number of sample.\n",
    "4. The length of each image.\n",
    "5. The sliding step size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2dd08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Read raw microphone data\n",
    "\n",
    "Provide the raw microphone data local directory path in the following text box, the raw data will be loaded.\n",
    "\n",
    "(*Example*: ../data/microphone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9a205-f05c-48df-999c-0cd3139d2668",
   "metadata": {},
   "source": [
    "#### Press ‚ñ∂Ô∏è to Select Raw Microphone Data to Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c412269",
   "metadata": {
    "has_explanation": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create text input widget for path input\n",
    "path_input = widgets.Text(\n",
    "    description='Directory Path:',\n",
    "    placeholder='Paste or type the directory path here',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Create a button to confirm the directory selection\n",
    "confirm_button = widgets.Button(description=\"Confirm Path\")\n",
    "\n",
    "text = widgets.Text(value=\"../data/microphone\", description=\"Example\")\n",
    "\n",
    "# Variable to hold the path\n",
    "# Default path: ../Data/Microphone\n",
    "global directory_path\n",
    "directory_path = \"\"\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "# Define button click event handler\n",
    "def on_confirm_button_clicked(b):\n",
    "    global directory_path\n",
    "    directory_path = path_input.value\n",
    "    print(f\"Path selected: {directory_path}\")\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.mat'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            mat_data = loadmat(file_path)\n",
    "            \n",
    "            data_name = os.path.splitext(filename)[0]\n",
    "            print(data_name+' is loaded')\n",
    "            data_dict[data_name] = mat_data\n",
    "            \n",
    "confirm_button.on_click(on_confirm_button_clicked)\n",
    "\n",
    "# Display widgets\n",
    "display(text)\n",
    "display(path_input, confirm_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9efe1-92ae-48f6-951a-4be39386f649",
   "metadata": {},
   "source": [
    "#### Press ‚ñ∂Ô∏è to Display the Selected Raw Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed2ad2-6c1a-4f6d-955c-f19893753990",
   "metadata": {
    "has_explanation": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x = list(data_dict.keys())[0]\n",
    "y = list(data_dict[x].keys())[3]\n",
    "\n",
    "plt.figure(figsize=(8,5),dpi=100)\n",
    "plt.plot(data_dict[x][y])\n",
    "plt.title(\"Raw microphone signal sample\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45627b75",
   "metadata": {},
   "source": [
    "## Removing noise through STFT and FFT filtering\n",
    "\n",
    "Provide the image data **save folder directory path**. **The number of sample**, **length of each image**, **sliding step size**, and **the number of the image** can be modified to preview the output of the image data. Once all the variables are determined, the user can convert the signal data to image data.\n",
    "\n",
    "(*Example*: ../data/image_data/microphone/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd95191-d81f-420e-97bd-776e24728ab0",
   "metadata": {},
   "source": [
    "### The 2-D microphone image data modification and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8731b978-29b2-427a-bbc0-77b5dff3de31",
   "metadata": {},
   "source": [
    "#### Press ‚ñ∂Ô∏è to Select the Save Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b25b6-eea2-4914-b9a7-4a835960eec5",
   "metadata": {
    "has_explanation": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create text input widget for path input\n",
    "path_input = widgets.Text(\n",
    "    description='Save Forlder Directory Path:',\n",
    "    placeholder='Paste or type the directory path here',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Create a button to confirm the directory selection\n",
    "confirm_button = widgets.Button(description=\"Confirm Path\")\n",
    "text = widgets.Text(value=\"../data/image_data/microphone/\", description=\"Example\")\n",
    "\n",
    "# Variable to hold the path\n",
    "# Default path: \n",
    "# /Users/zyichao/Desktop/phd/Codes_Additive_Manufacturing/Two_Sensor_Fusion_JMP/Data/photodiode\n",
    "global save_folder\n",
    "save_folder = \"\"\n",
    "\n",
    "# Define button click event handler\n",
    "def on_confirm_button_clicked(b):\n",
    "    global save_folder\n",
    "    save_folder = path_input.value\n",
    "    print(f\"Path selected: {directory_path}\")\n",
    "            \n",
    "confirm_button.on_click(on_confirm_button_clicked)\n",
    "\n",
    "# Display widgets\n",
    "display(text)\n",
    "display(path_input, confirm_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dff873-094e-4196-97d3-a2991103e659",
   "metadata": {},
   "source": [
    "#### Press ‚ñ∂Ô∏è to Modify the Preprocessing Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e955b45-5346-4efa-9032-234a92d43384",
   "metadata": {
    "has_explanation": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def microphone(a,b,c,d):\n",
    "    ## a:file position, \n",
    "    ##b:size of each image, c:Sliding step size, d:the number of the image\n",
    "    \n",
    "    filename = list(data_dict.keys())[a]\n",
    "    parts = filename.split('_')\n",
    "    mcp_grade = int(parts[1]) \n",
    "    mcp_num = int(parts[2])\n",
    "    \n",
    "    filtered_data = []\n",
    "    data_all = []\n",
    "\n",
    "    for j in range(1,18):\n",
    "        x = data_dict[filename]['cDAQ1Mod2ai'+str(j)]\n",
    "#         print(x.shape)\n",
    "\n",
    "        data = x\n",
    "\n",
    "        data_1 = data.reshape(len(data),)\n",
    "        N = len(data)\n",
    "        fs = 10000  # Sampling frequency\n",
    "        t = np.arange(1, N+1)  # Sampling moments\n",
    "\n",
    "        # FFT of the data\n",
    "        data_fft = np.fft.fft(data_1)\n",
    "        df = fs / N  # Frequency sampling interval\n",
    "        data_f = np.arange(0, N) * df\n",
    "\n",
    "        # Find the indices where frequency is less than 4000 Hz\n",
    "        id0 = np.where(data_f < 4000)[0]\n",
    "        id0_len = len(id0)\n",
    "\n",
    "        # Filtering to make the low frequency part as 0\n",
    "        data_fft[id0] = 0\n",
    "        data_fft[-id0_len+1:] = 0\n",
    "\n",
    "        # Inverse FFT after filtering\n",
    "        data_ifft = np.real(np.fft.ifft(data_fft))\n",
    "        \n",
    "        data_ifft = data_ifft[1000:-1000]\n",
    "        \n",
    "        indices = np.where(data_ifft > 0.1)[0]\n",
    "        first_index = indices[0]\n",
    "        last_index = indices[-1]\n",
    "        \n",
    "        first_third_index = int((last_index - first_index)/3) + first_index\n",
    "        second_third_index = int(2*(last_index - first_index)/3) + first_index\n",
    "\n",
    "        first_valid_indices = indices[indices < first_third_index]\n",
    "        first_valid_indice = first_valid_indices[-1]\n",
    "        \n",
    "        second_valid_indices_start = indices[indices > first_third_index]\n",
    "        second_valid_indice_start = second_valid_indices_start[0]\n",
    "        second_valid_indices_end = indices[indices < second_third_index]\n",
    "        second_valid_indice_end = second_valid_indices_end[-1]\n",
    "        \n",
    "        third_valid_indices = indices[indices > second_third_index]\n",
    "        third_valid_indice = third_valid_indices[0]\n",
    "        \n",
    "        y = np.concatenate([data_ifft[first_index:first_valid_indice],\n",
    "                            data_ifft[second_valid_indice_start:second_valid_indice_end],\n",
    "                            data_ifft[third_valid_indice:last_index]])\n",
    "        filtered_data.append(y)\n",
    "        \n",
    "    n = 32  # Size of each image\n",
    "\n",
    "    for m in range(1, 18):  # based on each layer, 17 layers in total\n",
    "        data = filtered_data[m-1]\n",
    "        N = len(data)\n",
    "        W = b * b  # Window size\n",
    "        # c: Sliding step size, middle value 10 * 10000 // 520 = 192, range from 50 to 300\n",
    "        hdcs = (N - W) // c + 1  # Number of sliding steps \n",
    "\n",
    "        for k in range(1, hdcs + 1):\n",
    "            one_data = data[(k-1) * c: (k-1) * c + W]\n",
    "            # Normalize and scale the data to 0-255\n",
    "            one_data = np.round((one_data - np.min(one_data)) / (np.max(one_data) - np.min(one_data)) * 255).astype(np.uint8)\n",
    "            # Reshape the image\n",
    "            one_data = one_data.reshape((b, b)) \n",
    "            \n",
    "            data_all.append(one_data)\n",
    "\n",
    "    \n",
    "    # e_value = 5 if e else 0\n",
    "    # result = a + b + c + d + e_value\n",
    "    result = filename\n",
    "    print(f\"The selected sample name: {result}\")\n",
    "    return plt.imshow(data_all[d],cmap='gray')\n",
    "    \n",
    "interact(microphone,\n",
    "         a=widgets.IntSlider(min=0, max=len(data_dict.keys())-1, step=1, value=0, description='The number of sample:',style={'description_width': 'initial'}),\n",
    "         b=widgets.IntSlider(min=32, max=128, step=1, value=0, description='Length of each image:',style={'description_width': 'initial'}),\n",
    "         c=widgets.IntSlider(min=50, max=300, step=1, value=0, description='Sliding step size:',style={'description_width': 'initial'}),\n",
    "         d=widgets.IntSlider(min=0, max=100, step=1, value=0, description='The number of the image:',style={'description_width': 'initial'}),\n",
    "\n",
    "         # d=widgets.Dropdown(options=[('One', 1), ('Two', 2), ('Three', 3)], value=1, description='d:'),\n",
    "         # e=widgets.Checkbox(value=False, description='Include e (value 5)')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d53099-f5d3-47e6-8e9d-7314c161f6d8",
   "metadata": {},
   "source": [
    "### Save the modified image data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ae960-1e51-436e-9751-8d7ac2f5404f",
   "metadata": {},
   "source": [
    "#### Press ‚ñ∂Ô∏è to Save the Modified Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f4551",
   "metadata": {
    "editable": true,
    "has_explanation": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    folder_path = os.path.join(save_folder, str(i))\n",
    "    try:\n",
    "        os.makedirs(folder_path)\n",
    "        print(\"Directory \", folder_path, \" Created \")\n",
    "    except FileExistsError:\n",
    "        print(\"Directory \", folder_path, \" already exists\")\n",
    "\n",
    "for i in range(len(data_dict.keys())):\n",
    "    filename = list(data_dict.keys())[i]\n",
    "    parts = filename.split('_')\n",
    "    mcp_grade = int(parts[1]) \n",
    "    mcp_num = int(parts[2])\n",
    "    \n",
    "    filtered_data = []\n",
    "    data_all = []\n",
    "    for j in range(1,18):\n",
    "        x = data_dict[filename]['cDAQ1Mod2ai'+str(j)]\n",
    "#         print(x.shape)\n",
    "\n",
    "        data = x\n",
    "\n",
    "        data_1 = data.reshape(len(data),)\n",
    "        N = len(data)\n",
    "        fs = 10000  # Sampling frequency\n",
    "        t = np.arange(1, N+1)  # Sampling moments\n",
    "\n",
    "        # FFT of the data\n",
    "        data_fft = np.fft.fft(data_1)\n",
    "        df = fs / N  # Frequency sampling interval\n",
    "        data_f = np.arange(0, N) * df\n",
    "\n",
    "        # Find the indices where frequency is less than 4000 Hz\n",
    "        id0 = np.where(data_f < 4000)[0]\n",
    "        id0_len = len(id0)\n",
    "\n",
    "        # Filtering to make the low frequency part as 0\n",
    "        data_fft[id0] = 0\n",
    "        data_fft[-id0_len+1:] = 0\n",
    "\n",
    "        # Inverse FFT after filtering\n",
    "        data_ifft = np.real(np.fft.ifft(data_fft))\n",
    "        \n",
    "        data_ifft = data_ifft[1000:-1000]\n",
    "        \n",
    "        indices = np.where(data_ifft > 0.1)[0]\n",
    "        first_index = indices[0]\n",
    "        last_index = indices[-1]\n",
    "        \n",
    "        first_third_index = int((last_index - first_index)/3) + first_index\n",
    "        second_third_index = int(2*(last_index - first_index)/3) + first_index\n",
    "\n",
    "        first_valid_indices = indices[indices < first_third_index]\n",
    "        first_valid_indice = first_valid_indices[-1]\n",
    "        \n",
    "        second_valid_indices_start = indices[indices > first_third_index]\n",
    "        second_valid_indice_start = second_valid_indices_start[0]\n",
    "        second_valid_indices_end = indices[indices < second_third_index]\n",
    "        second_valid_indice_end = second_valid_indices_end[-1]\n",
    "        \n",
    "        third_valid_indices = indices[indices > second_third_index]\n",
    "        third_valid_indice = third_valid_indices[0]\n",
    "\n",
    "        # Plot the result\n",
    "        plt.plot(data_ifft)\n",
    "        plt.title(\"The acoustic signal after FFT filtering for \"+filename)\n",
    "        plt.axhline(0.1,color = 'r',linestyle='--')\n",
    "        plt.axhline(-0.1,color = 'r',linestyle='--')\n",
    "        plt.axvline(first_index,color = 'r')\n",
    "        plt.axvline(first_valid_indice,color = 'r')\n",
    "        plt.axvline(second_valid_indice_start,color = 'g')\n",
    "        plt.axvline(second_valid_indice_end,color = 'g')\n",
    "        plt.axvline(third_valid_indice,color = 'y')\n",
    "        plt.axvline(last_index,color = 'y')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        y = np.concatenate([data_ifft[first_index:first_valid_indice],\n",
    "                            data_ifft[second_valid_indice_start:second_valid_indice_end],\n",
    "                            data_ifft[third_valid_indice:last_index]])\n",
    "#         print(y.shape)\n",
    "        filtered_data.append(y)\n",
    "       \n",
    "    folder = save_folder + str(mcp_grade)\n",
    "    n = 32  # Size of each image\n",
    "\n",
    "    for m in range(1, 18):  # based on each layer, 17 layers in total\n",
    "        data = filtered_data[m-1]\n",
    "        N = len(data)\n",
    "        W = n * n  # Window size\n",
    "        hd = 10 * 10000 // 520  # Sliding step size\n",
    "        hdcs = (N - W) // hd + 1  # Number of sliding steps\n",
    "\n",
    "        for k in range(1, hdcs + 1):\n",
    "            one_data = data[(k-1) * hd: (k-1) * hd + W]\n",
    "            # Normalize and scale the data to 0-255\n",
    "            one_data = np.round((one_data - np.min(one_data)) / (np.max(one_data) - np.min(one_data)) * 255).astype(np.uint8)\n",
    "            # Reshape the image\n",
    "            one_data = one_data.reshape((n, n)) \n",
    "            \n",
    "            data_all.append(one_data)\n",
    "            # Save the image\n",
    "            image_filename = f'p_{mcp_num}_{m}_{k}.png'\n",
    "            image_path = os.path.join(folder, image_filename)\n",
    "            Image.fromarray(one_data).save(image_path)\n",
    "            \n",
    "    print(filename, \"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c85458e-4816-4ce0-ae7d-c5f20f3cc6e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "home-nav"
    ]
   },
   "source": [
    "### <center>[üè† Home](../../../../../welcomePage.ipynb)</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
