{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2e3e12",
   "metadata": {
    "tags": [
     "auto-generated-toc"
    ]
   },
   "source": [
    "## Table of Contents\n",
    "- [Convolutional Neural Network](#Convolutional-Neural-Network)\n  - [Import Libraries](#Import-Libraries)\n  - [Load data](#Load-data)\n  - [Data Description](#Data-Description)\n  - [Model Preparation](#Model-Preparation)\n  - [Model Preparation](#Model-Preparation)\n  - [Model Summary](#Model-Summary)\n  - [Model Training](#Model-Training)\n- [üè† Home](../../../../welcomePage.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a85a8a-6e66-41cf-9d9e-684cbdbb333f",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59ad484-5cbf-4302-bc16-4452866229c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Convolutional Neural Networks (CNNs) are a class of deep neural networks primarily used for processing and analyzing visual data. They are particularly effective in tasks involving images due to their ability to capture spatial hierarchies and learn intricate patterns.\n",
    "\n",
    "### Key Components of CNNs:\n",
    "\n",
    "1. **Convolutional Layers:** These layers apply convolution operations using learnable filters that slide over the input image, capturing local patterns and features.\n",
    "\n",
    "2. **Pooling Layers:** Pooling layers reduce the spatial dimensions of the feature maps produced by convolutional layers, while preserving important information.\n",
    "\n",
    "3. **Activation Functions:** Typically, ReLU (Rectified Linear Unit) is used to introduce non-linearity after convolution and pooling operations.\n",
    "\n",
    "4. **Fully Connected Layers:** These layers take the high-level features extracted by convolutional layers and use them for classification or regression tasks.\n",
    "\n",
    "### Applications of CNNs:\n",
    "\n",
    "- **Image Classification:** Assigning labels to images based on their content.\n",
    "  \n",
    "- **Object Detection:** Locating and classifying objects within images.\n",
    "\n",
    "- **Image Segmentation:** Dividing images into meaningful parts or objects.\n",
    "  \n",
    "- **Face Recognition:** Identifying and verifying individuals based on facial features.\n",
    "\n",
    "- **Medical Image Analysis:** Analyzing medical images for diagnostic purposes, such as detecting tumors or abnormalities.\n",
    "\n",
    "- **Video Analysis:** Understanding and processing video content, such as action recognition or tracking objects over time.\n",
    "\n",
    "### Why CNNs are Effective:\n",
    "\n",
    "CNNs leverage parameter sharing and spatial locality to efficiently learn and generalize from visual data. This makes them well-suited for tasks where the arrangement of pixels and features in images is crucial for accurate analysis.\n",
    "\n",
    "In this notebook, we'll implement and explore the application of CNNs for a specific task, demonstrating their effectiveness in handling complex visual data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c649d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a09b88-a9c8-4544-84c0-5d42f925f474",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to import the libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f8e55",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 5.394808,
     "end_time": "2022-09-04T08:11:26.453293",
     "exception": false,
     "start_time": "2022-09-04T08:11:21.058485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "print(\"Importing liblease wait...\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Video\n",
    "\n",
    "print(\"All libraries were imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7447851",
   "metadata": {
    "papermill": {
     "duration": 0.006109,
     "end_time": "2022-09-04T08:11:26.465374",
     "exception": false,
     "start_time": "2022-09-04T08:11:26.459265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6b85d-7b59-402b-b880-a029dc09e2bb",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to load and display the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7467ce-269d-49b8-98b0-a63376e62837",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from ipyfilechooser import FileChooser\n",
    "from IPython.display import display, clear_output,HTML\n",
    "\n",
    "# Create a FileChooser widget for selecting a folder\n",
    "folder_chooser = FileChooser()\n",
    "folder_chooser.show_only_dirs = True\n",
    "\n",
    "# Create a \"Select\" button widget\n",
    "select_button = widgets.Button(\n",
    "    description='Select',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to select folder',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Create a \"Show Data\" button widget\n",
    "show_data_button = widgets.Button(\n",
    "    description='Show Data',\n",
    "    disabled=True,  # Initially disabled\n",
    "    button_style='',\n",
    "    tooltip='Click to show data',\n",
    "    icon='eye'\n",
    ")\n",
    "\n",
    "# Output widget to display messages and images\n",
    "output = widgets.Output()\n",
    "image_display = widgets.Output()\n",
    "\n",
    "# Initialize the stored_files array\n",
    "stored_files = []\n",
    "\n",
    "# Function to handle \"Select\" button click\n",
    "def on_select_button_click(b):\n",
    "    global selected_folder\n",
    "    with output:\n",
    "        clear_output()\n",
    "        selected_folder = folder_chooser.selected\n",
    "        if not selected_folder:\n",
    "            print(\"No folder selected. Please select a folder.\")\n",
    "            return\n",
    "\n",
    "        global stored_files\n",
    "        stored_files = [os.path.join(selected_folder, f) for f in os.listdir(selected_folder) if os.path.isdir(os.path.join(selected_folder, f))]\n",
    "        show_data_button.disabled = False if stored_files else True\n",
    "        print(f\"Folders from '{selected_folder}' have been loaded.\")\n",
    "\n",
    "# Function to display one image from each folder\n",
    "def on_show_data_button_click(b):\n",
    "    with image_display:\n",
    "        clear_output(wait=True)\n",
    "        for folder in stored_files:\n",
    "            folder_name = os.path.basename(folder)\n",
    "            for file_name in os.listdir(folder):\n",
    "                if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                    img_path = os.path.join(folder, file_name)\n",
    "                    image = Image.open(img_path)\n",
    "                    # Resize image and add title\n",
    "                    image = image.resize((300, 300))  # Resize the image to 300x300 pixels\n",
    "                    display(HTML(f\"<h3>{folder_name}</h3>\"))  # Display the folder name as a title\n",
    "                    display(image)\n",
    "                    break  # Display only one image per folder\n",
    "\n",
    "# Attach the functions to the button widgets\n",
    "select_button.on_click(on_select_button_click)\n",
    "show_data_button.on_click(on_show_data_button_click)\n",
    "\n",
    "# Display the folder chooser, buttons, and output widgets\n",
    "display(folder_chooser)\n",
    "display(select_button)\n",
    "display(show_data_button)\n",
    "display(output)\n",
    "display(image_display)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cbb5d5-acfe-4d93-9327-34b884ec4ea0",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to register the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240865d-621e-45c8-9b89-9fddc928d646",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = selected_folder\n",
    "print (\"Data is registered successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c8a121",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee98fca",
   "metadata": {},
   "source": [
    "The dataset was collected with a RPI and a Ender 3 pro in order to detect the anomalies during the printing process. It consists of 759 defected print images and 798 non-defected print images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa4c00-984b-4084-adc9-a47ab4ecf1cc",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to display the number of images per class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5658b62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_files_in_folders(data_dir):\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        if root == data_dir:\n",
    "            continue  # Skip the starting directory itself\n",
    "        folder_name = os.path.relpath(root, data_dir)\n",
    "        file_count = len(files)\n",
    "        print(f\"Folder '{folder_name}' contains {file_count} files\")\n",
    "\n",
    "count_files_in_folders(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800385d-cb57-45e4-a277-4bea01db8e35",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c882c4f-58a3-46cd-a786-d170fb0f3e33",
   "metadata": {},
   "source": [
    "In the context of training a Convolutional Neural Network (CNN) in frameworks like TensorFlow or Keras, specifying the batch size, image height, and image width is crucial for several reasons:\n",
    "\n",
    "**Batch Size:**\n",
    "- **Training Efficiency:** Specifying a batch size determines how many samples (images and their corresponding labels) are propagated through the network at once during each training iteration.\n",
    "- **Memory Management:** It helps in managing memory usage, as processing too many images at once might exceed available memory, while too few may not fully utilize hardware resources.\n",
    "\n",
    "**Image Dimensions (Height and Width):**\n",
    "- **Input Shape:** CNNs expect input data to have a consistent shape. Specifying image height and width ensures that all images in the dataset are resized or processed to a uniform size before being fed into the network.\n",
    "- **Convolutional Layer Requirements:** Convolutional layers in CNNs require input images to have a defined shape (height, width, number of channels), which is specified in the first layer of the model (e.g., `input_shape=(height, width, channels)`).\n",
    "\n",
    "**Model Architecture Requirements:**\n",
    "- **Dimension Matching:** Each layer in the CNN expects input data in a specific format (e.g., images as matrices of pixel values). Specifying dimensions ensures that the data fed into the network matches these expectations.\n",
    "- **Performance and Accuracy:** Correctly sized input images and appropriate batch sizes help in achieving optimal performance and accuracy during training and inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4acec2-fb6d-4a7c-93ef-f1234c435b78",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to specify the batch size, image height, and the image width.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52f472",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "batch_size = None\n",
    "img_height = None\n",
    "img_width = None\n",
    "\n",
    "# Function to handle button click event\n",
    "def on_button_click(b):\n",
    "    global batch_size, img_height, img_width\n",
    "    batch_size_input = text_box_batch_size.value.strip()\n",
    "    img_height_input = text_box_img_height.value.strip()\n",
    "    img_width_input = text_box_img_width.value.strip()\n",
    "    \n",
    "    if batch_size_input == '' or img_height_input == '' or img_width_input == '':\n",
    "        with output_widget:\n",
    "            output_widget.clear_output()\n",
    "            print(\"Error: Please enter values for all fields.\")\n",
    "    else:\n",
    "        try:\n",
    "            batch_size = int(batch_size_input)\n",
    "            img_height = int(img_height_input)\n",
    "            img_width = int(img_width_input)\n",
    "            output_widget.clear_output()\n",
    "            with output_widget:\n",
    "                print(f\"Variables set successfully: batch_size={batch_size}, img_height={img_height}, img_width={img_width}\")\n",
    "        except ValueError:\n",
    "            with output_widget:\n",
    "                output_widget.clear_output()\n",
    "                print(\"Please enter valid integers.\")\n",
    "\n",
    "# Create widgets\n",
    "text_box_batch_size = widgets.Text(placeholder='Enter batch size', description='Batch Size:',layout=widgets.Layout(width='300px'), style={'description_width': '150px'})\n",
    "text_box_img_height = widgets.Text(placeholder='Enter image height', description='Image Height:',layout=widgets.Layout(width='300px'), style={'description_width': '150px'})\n",
    "text_box_img_width = widgets.Text(placeholder='Enter image width', description='Image Width:',layout=widgets.Layout(width='300px'), style={'description_width': '150px'})\n",
    "button_set_variables = widgets.Button(description='Set Variables')\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "# Assign button click event\n",
    "button_set_variables.on_click(on_button_click)\n",
    "\n",
    "# Display widgets\n",
    "display(text_box_batch_size, text_box_img_height, text_box_img_width, button_set_variables, output_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec520ad-f55e-4579-a8e7-56c1ac71766f",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to split the data into training and testing sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2fe95",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define global variables\n",
    "validation_split = None\n",
    "\n",
    "# Function to handle button click event\n",
    "def on_button_click(b):\n",
    "    global validation_split\n",
    "    validation_split = slider.value / 100.0  # Convert slider value to a fraction\n",
    "    output_widget.clear_output()\n",
    "    with output_widget:\n",
    "        print(f\"Validation percentage set to: {validation_split:.0%}\");\n",
    "\n",
    "# Create widgets\n",
    "slider = widgets.IntSlider(value=20, min=0, max=90, step=1, description='Validation Split %:',layout=widgets.Layout(width='500px'), style={'description_width': '150px'})\n",
    "button_set_split = widgets.Button(description='Set Split')\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "# Assign button click event\n",
    "button_set_split.on_click(on_button_click)\n",
    "\n",
    "# Display widgets\n",
    "display(slider, button_set_split, output_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf75a45-257b-40d8-ba73-aaf8bc77eb2e",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to display the number of images in the training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceddd3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.361051,
     "end_time": "2022-09-04T08:11:29.871036",
     "exception": false,
     "start_time": "2022-09-04T08:11:26.509985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory = data_dir,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9965805-b01c-4e94-a951-ff9ab68f1724",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to display the number of images in the testing sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f14657",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.131379,
     "end_time": "2022-09-04T08:11:30.009195",
     "exception": false,
     "start_time": "2022-09-04T08:11:29.877816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory = data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1504f1bd-9f58-471c-b501-4f8b8502071f",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to display the classes' names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2373ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.015474,
     "end_time": "2022-09-04T08:11:30.031571",
     "exception": false,
     "start_time": "2022-09-04T08:11:30.016097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(\"The data contains classes:\")\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1732fa6-c8d8-4540-9a99-c3e325145134",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to display some of the images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857489f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 1.919255,
     "end_time": "2022-09-04T08:11:31.957481",
     "exception": false,
     "start_time": "2022-09-04T08:11:30.038226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded88c40",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c440a-8aa6-477a-81f0-ff69ec3f5ab7",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to maximize model performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf8171",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.023865,
     "end_time": "2022-09-04T08:11:32.651249",
     "exception": false,
     "start_time": "2022-09-04T08:11:32.627384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training and validation data preparation for improved performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"Training and validation datasets processed for performance maximization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8946f9-a78b-43c4-a0ac-8889200e9d67",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to normalize the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c50fe4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.759352,
     "end_time": "2022-09-04T08:11:36.465702",
     "exception": false,
     "start_time": "2022-09-04T08:11:32.706350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255.0)\n",
    "\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "## Notice the pixel values are now in `[0,1]`.\n",
    "#print(np.min(first_image), np.max(first_image)) # To check if data was normalized correctly\n",
    "\n",
    "print (\"Training images data was normalized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384bad2-0f92-4b8d-a842-4acac4889919",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to define the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5b12a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.110155,
     "end_time": "2022-09-04T08:11:36.625219",
     "exception": false,
     "start_time": "2022-09-04T08:11:36.515064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "print (\"Model has been defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd23ec0-0c75-4f25-a8db-05127022c483",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to compile the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d6b620",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.027356,
     "end_time": "2022-09-04T08:11:36.663764",
     "exception": false,
     "start_time": "2022-09-04T08:11:36.636408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print (\"Model has been compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72b6e1",
   "metadata": {
    "papermill": {
     "duration": 0.016454,
     "end_time": "2022-09-04T08:11:36.499192",
     "exception": false,
     "start_time": "2022-09-04T08:11:36.482738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5ff1c-19e8-4181-b619-18c34ee5e260",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to display the model summary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31c35a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.021451,
     "end_time": "2022-09-04T08:11:36.695753",
     "exception": false,
     "start_time": "2022-09-04T08:11:36.674302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f6248-d19f-4351-872d-5850e2b22e29",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88d510-63f5-4eab-b3e1-6ee98221e8a8",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to train the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966d933",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 17.668163,
     "end_time": "2022-09-04T08:11:54.374237",
     "exception": false,
     "start_time": "2022-09-04T08:11:36.706074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs=15\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "print('')\n",
    "print('Model has been successfully trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1339ffb5-fc1d-4a7b-b861-a6f0d285f35a",
   "metadata": {},
   "source": [
    "### Validation and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add52392-be4c-4e36-a245-b8aa219a0c20",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to show the accuracies and the losses of the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16617972",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.319741,
     "end_time": "2022-09-04T08:11:54.712689",
     "exception": false,
     "start_time": "2022-09-04T08:11:54.392948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071d76cd-4349-477f-ba5c-41a1b99bc4b3",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to display the test accuracy and the test loss values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e634e8db-9dec-4187-8043-cdbb82c02b4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# verify the performance of the model\n",
    "loss, accuracy = model.evaluate(val_ds, verbose=0)\n",
    "print('Test accuracy :', accuracy)\n",
    "print('Test loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dcb5f3-ab76-4fb4-a475-e7bdbfe0964f",
   "metadata": {},
   "source": [
    "**Press ‚ñ∂ to display instances of the testing set predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae076b-0a8f-4ae9-836f-79d16d718f07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a batch from validation_ds to do some inference\n",
    "image_batch, label_batch = val_ds.as_numpy_iterator().next()\n",
    "\n",
    "# inference\n",
    "inference = model.predict_on_batch(image_batch)\n",
    "\n",
    "# apply softmax to convert logits to probabilities\n",
    "probabilities = tf.nn.softmax(inference, axis=-1).numpy()\n",
    "\n",
    "# show imgs and labels\n",
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(12):\n",
    "    ax = plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "    plt.title('Inference:{}, {:.0f}% Confidence\\nReal Label:{}'\n",
    "              .format(class_names[np.argmax(probabilities[i])], 100 * np.max(probabilities[i]), class_names[label_batch[i]]))\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887e582-41c2-4a92-9fca-2932aa426e31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "home-nav"
    ]
   },
   "source": [
    "### <center>[üè† Home](../../../../welcomePage.ipynb)</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 80.523063,
   "end_time": "2022-09-04T08:12:33.299441",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-04T08:11:12.776378",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
