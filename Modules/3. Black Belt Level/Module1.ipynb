{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4faba2c-a0be-4bed-b0da-fc48aa0735e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <font color = 'red'>1. *Introduction to Data in Digital Engineering (Handling Unstructured Data)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8028382",
   "metadata": {},
   "source": [
    "<font size = 3>In this module, we will focus on dealing with unstructured data, specifically in the context of engineering. One key aspect we will explore is how height map scans are essential in engineering applications. However, while the height map itself provides raw data, it is not inherently meaningful until we extract the relevant features from it. The importance of extracting these features lies in transforming the unstructured data into valuable information that can be used for analysis and decision-making. To improve this process, we will cover the tools and techniques that are essential for feature extraction, allowing us to derive actionable insights from the height map scans.\n",
    "</font>\n",
    "#### What is a 3D Scan?\n",
    "A 3D scan is a digital representation of the three-dimensional shape of an object. This technology captures the exact size and shape of physical objects using a device that records comprehensive data on its shape, color, and sometimes texture. The resulting scan forms a point cloud, or a collection of data points in three-dimensional space, which can be used to create a 3D model. These scans are utilized across various industries, including manufacturing, entertainment, healthcare, and archaeology, to create models for analysis, reproduction, or digital rendering.\n",
    "\n",
    "<center>\n",
    "    <img src=\"Module 1 Content/img/01.jpg\" alt=\"Alt Text\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "#### Objective\r\n",
    "\r\n",
    "The described training module is focused on developing key skills necessary for working with unstructured data throughout its entire processing lifecycle. While the 3D scan application is a specific example, the core concepts and techniques will be applicable to various types of unstructured data in different fields. This module aims to provide a comprehensive understanding of how to handle unstructured data, from initial raw data collection to meaningful feature extraction and analysis.\r\n",
    "\r\n",
    "#### - Cleaning Noise from Data:\r\n",
    "The module will cover techniques for identifying and removing irrelevant or extraneous data, similar to cleaning up noise in a 3D scan. This involves recognizing what constitutes useful information in a dataset and isolating the parts that are important for analysis. Whether it's removing unwanted noise in a height map or irrelevant data in other forms of unstructured data, this skill is crucial for ensuring that only relevant features are considered.\r\n",
    "\r\n",
    "#### - Data Calibration:\r\n",
    "Calibration techniques are an essential part of improving the quality of unstructured data. By applying mathematical methods such as regression to the raw data, you can adjust the data points to better fit the expected values or to correct for errors in the data collection process. This ensures that the resulting dataset is as accurate as possible and ready for further analysis.\r\n",
    "\r\n",
    "#### - Feature Extraction and Analysis:\r\n",
    "After cleaning and calibrating the data, the next step is extracting meaningful features from it. This is where the core value of unstructured data lies—by using advanced techniques to extract useful information, you can transform raw data into valuable insights. Whether fitting geometric shapes to 3D scan data or identifying key patterns in other types of unstructured data, this step is essential for turning noise into actionable knowledge.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe97524",
   "metadata": {},
   "source": [
    "#### Press ▶️ to Select 3D Data to Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95112adf-1a7f-4cea-b7dc-04010127b1d3",
   "metadata": {
    "has_explanation": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bb7a98f20345c2862656193fc2e6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Select file:', options=('./Module 1 Content/data\\\\Sin_T_1_Gr_1.csv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import Dropdown, interact\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "sys.path.append('Module 1 Content')  # Adjust the path as necessary\n",
    "\n",
    "from functions import *\n",
    "\n",
    "\n",
    "\n",
    "# Function to get a list of CSV files in the 'data' folder\n",
    "def get_csv_files():\n",
    "    data_dir = './Module 1 Content/data'  # Directory where CSV files are located\n",
    "    return [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Function to load the selected CSV file into a global variable\n",
    "def load_selected_csv(file):\n",
    "    global data\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Loading {file}...\")\n",
    "    data = load_data(file)  # Assuming load_data is a custom function to load CSV into data\n",
    "    plot(data)  # Assuming plot is a custom function to plot data\n",
    "\n",
    "# Dropdown widget to select a CSV file\n",
    "csv_files = get_csv_files()\n",
    "file_selector = Dropdown(options=csv_files, description=\"Select file:\")\n",
    "\n",
    "# Interactive widget to update the global variable based on selected file\n",
    "interact(load_selected_csv, file=file_selector);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a85579f-55f4-4ce7-8d94-3947b8061daf",
   "metadata": {},
   "source": [
    "### Plot Calibration Instructions\r\n",
    "\r\n",
    "In the context of plot calibration, the primary goal is to ensure that the data we are working with is accurate and aligned, especially when it comes to working with physical objects or scans. The process described here involves using a flat and stable region of the scan, often referred to as the \"bed,\" to help correct any distortions or biases present in the entire dataset.\r\n",
    "\r\n",
    "The first step is to **select the bed region**. This flat area is crucial because it serves as a reference for what should be level or stable in the scan. By defining this region accurately using sliders, we ensure that we are working with a reliable base point from which to assess the rest of the data.\r\n",
    "\r\n",
    "Next, we **fit a linear regression line** to the bed region. Why do we do this? A linear regression helps us capture the overall slope of the bed. Since the bed is expected to be flat, any deviation from flatness (such as tilting or unevenness) will be reflected in the slope of this line. Fitting a regression line allows us to quantify this slope mathematically, providing us with a precise measurement of how much the bed might be tilted or uneven.\r\n",
    "\r\n",
    "Finally, we **apply slope calibration**. After obtaining the slope from the bed region, we use it to adjust the entire scan or plot. This ensures that any inclinations or distortions throughout the data are corrected based on the flat and stable bed reference. By doing this, we align the whole dataset, ensuring that any measurements or further analysis are accurate and reliable, free from any bias introduced by an uneven surface or setup.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1db4ea-c838-4a48-842d-78375afab95d",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"Module 1 Content/img/02.jpg\" alt=\"Alt Text\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c8a4c-dc26-45f6-8432-49a5ab44699a",
   "metadata": {},
   "source": [
    "#### Press ▶️ to Select Bed Region for Calibration <font color = 'green'>(Scan Bed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a3fbe49-5a99-4f9a-9cd3-10ba38a5e9c8",
   "metadata": {
    "has_explanation": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888ce45559174ec389b19a6de36b75e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-6.696, description='Minimum', max=1.866, min=-6.696, step=0.01), Floa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import FloatSlider, interactive\n",
    "from IPython.display import display\n",
    "\n",
    "# Assuming 'data' is your original 2D numpy array\n",
    "data_temp = np.copy(data)  # Use the original data directly with NaNs preserved\n",
    "\n",
    "# Flatten the data to prepare for the scatter plot\n",
    "points = data_temp.flatten()\n",
    "\n",
    "# Initialize global variables\n",
    "global upper_bound_plate, lower_bound_plate\n",
    "upper_bound_plate = np.nanmax(points)\n",
    "lower_bound_plate = np.nanmin(points)\n",
    "\n",
    "def update_plots(y1, y2):\n",
    "    global upper_bound_plate, lower_bound_plate\n",
    "    # Update global variables with the slider values\n",
    "    upper_bound_plate = y2\n",
    "    lower_bound_plate = y1\n",
    "    \n",
    "    # Create a mask for values outside the slider range, respecting NaNs\n",
    "    mask = (data_temp < y1) | (data_temp > y2)\n",
    "    filtered_data = np.copy(data_temp)\n",
    "    filtered_data[mask] = np.nan  # Set values outside the range to NaN\n",
    "    \n",
    "    # Plotting using subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Scatter plot on the first subplot\n",
    "    valid_points = ~np.isnan(points)  # Mask to remove NaNs for the scatter plot\n",
    "    ax1.scatter(np.arange(len(points))[valid_points], points[valid_points], marker='o', linestyle='-', s=0.01)\n",
    "    ax1.set_title('Flattened Array Values')\n",
    "    ax1.set_xlabel('Index')\n",
    "    ax1.set_ylabel('Height Value')\n",
    "    ax1.grid(True)\n",
    "    ax1.axhline(y=lower_bound_plate, color='r', linestyle='--')\n",
    "    ax1.axhline(y=upper_bound_plate, color='g', linestyle='--')\n",
    "    \n",
    "    # Heatmap on the second subplot\n",
    "    sns.heatmap(filtered_data, cmap='viridis', cbar=False, ax=ax2)\n",
    "    ax2.set_aspect(aspect= 'equal')\n",
    "    ax2.set_title('Filtered Data Heatmap')\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Set up sliders for the interactive lines\n",
    "y1_slider = FloatSlider(min=np.nanmin(points), max=np.nanmax(points), step=0.01, value=np.nanmin(points), description='Minimum')\n",
    "y2_slider = FloatSlider(min=np.nanmin(points), max=np.nanmax(points), step=0.01, value=np.nanmax(points), description='Maximum')\n",
    "\n",
    "interactive_plot = interactive(update_plots, y1=y1_slider, y2=y2_slider)\n",
    "output = interactive_plot.children[-1]\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6dc59d-8cab-4d1b-9d31-469dafa0ea4c",
   "metadata": {},
   "source": [
    "#### Press ▶️ to Start Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c131ba42-6aef-4bbf-a4bd-5a346107ed59",
   "metadata": {
    "has_explanation": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98b27f730d348a29c4ce44cd094da1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Calibrate Scan', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c023641ffe04ee9bf59b8109e5325ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "button = widgets.Button(description=\"Calibrate Scan\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    global data  # Make sure to modify the global 'data' variable\n",
    "    with output:\n",
    "        data = correct_tilt(data, lower_bound_plate, upper_bound_plate)\n",
    "        print(\"Data has been updated.\")\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83347e7-5cb2-4586-b0d1-c473c5b7b3b5",
   "metadata": {},
   "source": [
    "### Feature Extraction Instructions\r\n",
    "\r\n",
    "In the feature extraction process, the goal is to focus on the relevant part of the data while removing any unnecessary information that could distract from the analysis. The first step in this process is to **define the area** of interest.\r\n",
    "\r\n",
    "To do this, you carefully adjust the sliders to select the region of the plot that contains the part you are most concerned with, whether it's a specific feature or the object you're studying. This allows you to isolate the relevant data from the rest of the plot. The **bed**, which we previously calibrated, is not part of the area of interest, so it must be excluded from this selection. By removing the bed, we ensure that the data we're working with is focused on the specific object or feature of interest, allowing for a more precise analysis. \r\n",
    "\r\n",
    "This process helps us filter out irrelevant data and concentrate on what truly matters, ensuring that our subsequent steps in analysis, such as fitting models or extracting more detailed features, are based only on the data that is most important for the task at hand.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049bc9b-426a-4224-9cb7-d366bf2ffa42",
   "metadata": {},
   "source": [
    "#### Press ▶️ to select region of interest for feature extraction: <font color = 'red'>(Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "493b28aa-eced-4f56-a9e0-7939fd4e80cf",
   "metadata": {
    "has_explanation": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118ecf4cf0b94e409f3ffd11473772b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-4.850075325360749, description='Minimum', max=3.659646700863387, min=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import FloatSlider, interactive\n",
    "from IPython.display import display\n",
    "\n",
    "# Assuming 'data' is your original 2D numpy array\n",
    "data_temp = np.copy(data)  # Use the original data directly with NaNs preserved\n",
    "\n",
    "# Flatten the data to prepare for the scatter plot\n",
    "points = data_temp.flatten()\n",
    "\n",
    "# Initialize global variables\n",
    "global upper_bound, lower_bound\n",
    "upper_bound = np.nanmax(points)\n",
    "lower_bound = np.nanmin(points)\n",
    "\n",
    "def update_plots(y1, y2):\n",
    "    global upper_bound, lower_bound\n",
    "    # Update global variables with the slider values\n",
    "    upper_bound = y2\n",
    "    lower_bound = y1\n",
    "    \n",
    "    # Create a mask for values outside the slider range, respecting NaNs\n",
    "    mask = (data_temp < y1) | (data_temp > y2)\n",
    "    filtered_data = np.copy(data_temp)\n",
    "    filtered_data[mask] = np.nan  # Set values outside the range to NaN\n",
    "    \n",
    "    # Plotting using subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Scatter plot on the first subplot\n",
    "    valid_points = ~np.isnan(points)  # Mask to remove NaNs for the scatter plot\n",
    "    ax1.scatter(np.arange(len(points))[valid_points], points[valid_points], marker='o', linestyle='-', s=0.01)\n",
    "    ax1.set_title('Flattened Array Values')\n",
    "    ax1.set_xlabel('Index')\n",
    "    ax1.set_ylabel('Height Value')\n",
    "    ax1.grid(True)\n",
    "    ax1.axhline(y=lower_bound, color='r', linestyle='--')\n",
    "    ax1.axhline(y=upper_bound, color='g', linestyle='--')\n",
    "    \n",
    "    # Heatmap on the second subplot\n",
    "    sns.heatmap(filtered_data, cmap='viridis', cbar=False, ax=ax2)\n",
    "    ax2.set_aspect(aspect= 'equal')\n",
    "    ax2.set_title('Filtered Data Heatmap')\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Set up sliders for the interactive lines\n",
    "y1_slider = FloatSlider(min=np.nanmin(points), max=np.nanmax(points), step=0.01, value=np.nanmin(points), description='Minimum')\n",
    "y2_slider = FloatSlider(min=np.nanmin(points), max=np.nanmax(points), step=0.01, value=np.nanmax(points), description='Maximum')\n",
    "\n",
    "interactive_plot = interactive(update_plots, y1=y1_slider, y2=y2_slider)\n",
    "output = interactive_plot.children[-1]\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2383451-4ca0-4f6b-b8a1-a948b9049118",
   "metadata": {},
   "source": [
    "### Rationale for Image Conversion in Feature Extraction\r\n",
    "\r\n",
    "In engineering, rather than developing custom algorithms from scratch, we often seek existing, well-established solutions that can be applied to our specific problem. This approach is both efficient and practical, allowing us to leverage the expertise and resources of the broader engineering and research communities.\r\n",
    "\r\n",
    "In the case of feature extraction from 3D scan data, the decision to **convert the scan into an image format** is driven by the need to simplify the data for analysis. By transforming the 3D data into a 2D image, we can more easily apply a range of proven image processing techniques that are specifically designed to work with visual data. This conversion step is essential because it facilitates the use of tools and methods that have been optimized for shape recognition and feature extraction.\r\n",
    "\r\n",
    "The choice to utilize **image processing tools** is based on the availability of robust, efficient libraries within the image processing field. Python, in particular, offers a wide range of well-developed libraries for image-based feature extraction. These libraries, such as OpenCV and scikit-image, are extensively used in both academic and industrial applications for tasks such as edge detection, pattern recognition, and shape fitting. By converting the 3D scan into an image, we can take full advantage of these advanced, widely-used tools, allowing us to focus on solving the problem at hand without the need for reinventing the wheel.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559819dd-b138-4d32-90cb-ef0d867f860f",
   "metadata": {},
   "source": [
    "### Explanation of the Code and its Functionality\r\n",
    "\r\n",
    "This code is designed to extract and fit geometric shapes to a binary image, which represents data that has been processed for feature extraction. The following steps outline the core functionality:\r\n",
    "\r\n",
    "1. **Binary Data Preparation**: The data is first thresholded into a binary format, where pixels falling within a certain intensity range are marked as `1` (white), while all other pixels are marked as `0` (black). This step essentially isolates the relevant features from the background.\r\n",
    "\r\n",
    "2. **Contour Detection**: Once the binary data is prepared, the contours in the image are identified using the `cv2.findContours()` function. Contours represent the boundaries of connected regions in the binary image. The largest contour is selected for further analysis because it is assumed to correspond to the object of interest.\r\n",
    "\r\n",
    "3. **Shape Fitting**: Based on the selected shape (Rectangle, Triangle, Square, Circle, or Ellipse), the code fits a specific geometric model to the detected contour.\r\n",
    "\r\n",
    "   - **Rectangle**: The code uses `cv2.minAreaRect()` to fit a rectangle to the contour, and then the rectangle’s corners are extracted and scaled to physical units (in millimeters). The width and height of the rectangle are printed as part of the output.\r\n",
    "   \r\n",
    "   - **Triangle**: The function `cv2.minEnclosingTriangle()` fits the smallest enclosing triangle around the contour. The vertices of the triangle are extracted and scaled, and the coordinates of these vertices are printed.\r\n",
    "   \r\n",
    "   - **Square**: Similar to the rectangle fitting, a square is fitted by taking the largest side of the bounding rectangle and using that length for all sides. The square is placed at the center of the contour, and its side length is printed.\r\n",
    "   \r\n",
    "   - **Circle**: The function `cv2.minEnclosingCircle()` is used to fit the smallest enclosing circle to the contour. The center and radius of the circle are calculated, scaled, and displayed in the output.\r\n",
    "\r\n",
    "   - **Ellipse**: If the contour has at least 5 points, the code uses `cv2.fitEllipse()` to fit an ellipse to the contour. The center, axes (major and minor), and the orientation angle of the ellipse are calculated, scaled, and displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bad81e-c958-441d-adfa-f3c6654fd7bb",
   "metadata": {},
   "source": [
    "#### Press ▶️ to select feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ff317b2-b0cf-4213-9eee-873b5fcbf33b",
   "metadata": {
    "has_explanation": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e988e26cfc64ca7bd3faa2c5067f2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Shape:', options=('Rectangle', 'Triangle', 'Square', 'Ellipse', 'Circle')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9712839e45483c8f6a4a7d75f6434f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon, Ellipse\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the scale factor (0.02 mm per pixel)\n",
    "scale_factor = 0.02\n",
    "\n",
    "# Define the function to execute upon shape and epsilon selection\n",
    "def fit_and_plot(shape, epsilon_factor):\n",
    "    binary_data = np.where((data >= lower_bound) & (data <= upper_bound), 1, 0)\n",
    "    contours, _ = cv2.findContours(binary_data.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        epsilon = epsilon_factor * cv2.arcLength(largest_contour, True)\n",
    "        approx = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(binary_data, cmap='gray', origin='lower', extent=[0, binary_data.shape[1] * scale_factor, 0, binary_data.shape[0] * scale_factor])\n",
    "        \n",
    "        if shape == 'Rectangle':\n",
    "            rect = cv2.minAreaRect(approx)\n",
    "            box = cv2.boxPoints(rect) * scale_factor\n",
    "            plt.gca().add_patch(Polygon(box, closed=True, color='red', fill=False, linewidth=2))\n",
    "            print(f\"Rectangle width: {rect[1][0] * scale_factor} mm, height: {rect[1][1] * scale_factor} mm\")\n",
    "        \n",
    "        elif shape == 'Triangle':\n",
    "            triangle = cv2.minEnclosingTriangle(approx)[1] * scale_factor\n",
    "            plt.gca().add_patch(Polygon(triangle[0], closed=True, color='blue', fill=False, linewidth=2))\n",
    "            print(\"Triangle vertices:\", triangle[0])\n",
    "        \n",
    "        elif shape == 'Square':\n",
    "            rect = cv2.minAreaRect(approx)\n",
    "            side = max(rect[1]) * scale_factor\n",
    "            center = np.array(rect[0]) * scale_factor\n",
    "            box = np.array([\n",
    "                [center[0] - side / 2, center[1] - side / 2],\n",
    "                [center[0] + side / 2, center[1] - side / 2],\n",
    "                [center[0] + side / 2, center[1] + side / 2],\n",
    "                [center[0] - side / 2, center[1] + side / 2],\n",
    "            ])\n",
    "            plt.gca().add_patch(Polygon(box, closed=True, color='green', fill=False, linewidth=2))\n",
    "            print(f\"Square side length: {side} mm\")\n",
    "        \n",
    "        elif shape == 'Circle':\n",
    "            center, radius = cv2.minEnclosingCircle(approx)\n",
    "            center_scaled = np.array(center) * scale_factor\n",
    "            radius_scaled = radius * scale_factor\n",
    "            circle_patch = plt.Circle(center_scaled, radius_scaled, color='orange', fill=False, linewidth=2)\n",
    "            plt.gca().add_patch(circle_patch)\n",
    "            print(f\"Circle radius: {radius_scaled} mm\")\n",
    "        \n",
    "        elif shape == 'Ellipse':\n",
    "            if len(approx) >= 5:  # Ensure there are at least 5 points to fit an ellipse\n",
    "                ellipse = cv2.fitEllipse(approx)\n",
    "                ellipse_center = np.array(ellipse[0]) * scale_factor\n",
    "                axes = np.array(ellipse[1]) * scale_factor\n",
    "                angle = ellipse[2]\n",
    "                plt.gca().add_patch(Ellipse(xy=ellipse_center, width=axes[0], height=axes[1], angle=angle, edgecolor='purple', fill=False, linewidth=2))\n",
    "                print(f\"Ellipse center: {ellipse_center} mm, axes: {axes} mm, angle: {angle}\")\n",
    "            else:\n",
    "                print(\"Not enough points to fit an ellipse.\")\n",
    "\n",
    "\n",
    "        plt.xlabel('X (mm)')\n",
    "        plt.ylabel('Y (mm)')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No contours found in the binary image.\")\n",
    "\n",
    "# Widgets for shape and epsilon factor selection\n",
    "shape_selector = widgets.Dropdown(options=['Rectangle', 'Triangle', 'Square', 'Ellipse', 'Circle'], description='Shape:')\n",
    "epsilon_slider = widgets.FloatSlider(value=0.01, min=0.0, max=0.1, step=0.005, description='Epsilon Factor:', readout_format='.3f')\n",
    "\n",
    "# Link widgets to the function\n",
    "interactive_plot = widgets.interactive_output(fit_and_plot, {'shape': shape_selector, 'epsilon_factor': epsilon_slider})\n",
    "\n",
    "# Display the widgets and the plot\n",
    "display(widgets.VBox([shape_selector, epsilon_slider]), interactive_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe139152-98b1-4dbf-98f9-088545322a44",
   "metadata": {},
   "source": [
    "The 3D scans performed by technicians or engineers initially captured valuable data, but in their raw, unstructured form, they held little meaning. The data was essentially a collection of points and measurements without context, and without further processing, it wasn't immediately useful for decision-making or analysis. This is common in engineering where raw scans or measurements often lack direct applicability without additional interpretation.\r\n",
    "\r\n",
    "However, once we applied techniques to extract meaningful features and metrics from the scans—such as fitting geometric shapes, calibrating for distortions, and isolating areas of interest—the raw data transformed into actionable insights. These processed metrics now provide a structured, quantitative representation of the scanned object or system, which can be integrated with other engineering parameters.\r\n",
    "\r\n",
    "By joining the extracted features with existing engineering data, we can now generate a more complete understanding of the object being studied. This allows for better decision-making and more precise measurements. Instead of working with ambiguous, unprocessed scan data, engineers can now rely on refined, meaningful metrics that provide a clearer, more accurate picture of the object's characteristics, leading to improved performance and more informed analysis.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
