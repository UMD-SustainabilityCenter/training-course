{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-based feature fusion and quality monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, a feature-level multi-sensor fusion method based on 2D CNN is developed for quality monitoring using the converted acoustic and photodiode images as the inputs. In this work, a 10-layer CNN was developed to identify the three part quality levels by fusing two types of sensing data. There are 4 convolutional layers, 4 max-pooling layers, and 2 fully-connected layers in the proposed CNN model. Additionally, a rectified linear units (ReLU) activation function and a dropout strategy are applied following the first fully connected layer to mitigate the overfitting problem. The number of kernels of convolutional layer 1 to 4 are 16, 32, 32, and 64, respectively. The kernel size of each convolutional operation is 5 × 5 and the step size is 1. The kernel size of all the max-pooling layers is 2 × 2 and the step size is 2. The number of output features of the two fully-connected layers are 256 and 3, respectively.\n",
    "\n",
    "<img src=\"./figures/CNN.png\" alt=\"Sample Image\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets,transforms,models\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import pywt\n",
    "from ipywidgets import interact, IntSlider\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.onnx\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ignore specific UserWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torchvision.io.image')\n",
    "\n",
    "# from jupyterthemes import get_themes\n",
    "# import jupyterthemes as jt\n",
    "# from jupyterthemes.stylefx import set_nb_theme\n",
    "# !jt -t grade3 -T -N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load microphone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_path_microphone = []\n",
    "signal_microphone = os.listdir(\"../image_data/Microphone\")\n",
    "signal_microphone = signal_microphone[1:]\n",
    "for f, fsignal in enumerate(signal_microphone):\n",
    "    filepath = \"../image_data/Microphone\" + \"/\" + fsignal\n",
    "    filename = os.listdir(filepath)\n",
    "    for fname in filename:\n",
    "        ffpath = filepath + \"/\" + fname\n",
    "        path = [f, ffpath]\n",
    "        all_path_microphone.append(path)\n",
    "# print(len(all_path_microphone))\n",
    "\n",
    "def display_image_mcp(index):\n",
    "    img_path = all_path_microphone[index][1]\n",
    "    img = cv2.imread(img_path, 0)  \n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n",
    "\n",
    "interact(display_image_mcp, index=IntSlider(min=0, max=len(all_path_microphone)-1, step=1, value=0,description='Microphone'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load photodiode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_path_photodiode = []\n",
    "signal_photodiode = os.listdir(\"../image_data/Photodiode\")\n",
    "signal_photodiode = [x for x in signal_photodiode if x != '.DS_Store']\n",
    "\n",
    "for f, fsignal in enumerate(signal_photodiode):\n",
    "    filepath = os.path.join(\"../image_data/Photodiode\", fsignal)\n",
    "    filename = os.listdir(filepath)\n",
    "    # Additional check for .DS_Store inside subdirectories\n",
    "    filename = [file for file in filename if file != '.DS_Store']\n",
    "    for fname in filename:\n",
    "        ffpath = os.path.join(filepath, fname)\n",
    "        path = [f, ffpath]\n",
    "        all_path_photodiode.append(path)\n",
    "\n",
    "# all_path_photodiode = []\n",
    "# signal_photodiode = os.listdir(\"../image_data/Photodiode\")\n",
    "# signal_photodiode = signal_photodiode[1:]\n",
    "# for f, fsignal in enumerate(signal_photodiode):\n",
    "#     filepath = \"../image_data/Photodiode\" + \"/\" + fsignal\n",
    "#     filename = os.listdir(filepath)\n",
    "#     for fname in filename:\n",
    "#         ffpath = filepath + \"/\" + fname\n",
    "#         path = [f, ffpath]\n",
    "#         all_path_photodiode.append(path)\n",
    "# # print(len(all_path_photodiode))\n",
    "\n",
    "def display_image_phd(index):\n",
    "    img_path = all_path_photodiode[index][1]\n",
    "    img = cv2.imread(img_path, 0)  \n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n",
    "    \n",
    "interact(display_image_phd, index=IntSlider(min=0, max=len(all_path_photodiode)-1, step=1, value=0,description='Photodiode'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "data_x1_list = []\n",
    "data_x2_list = []\n",
    "data_y1_list = []\n",
    "data_y2_list = []\n",
    "\n",
    "for item1, item2 in zip(all_path_microphone,all_path_photodiode):\n",
    "    # print(item[0],item[1]) # 0 E:\\ml_datasets\\zhoucheng_data\\0\\0_1.png\n",
    "    if item1[0] == item2[0]:\n",
    "        img1=cv2.imread(item1[1],0)    \n",
    "        img2=cv2.imread(item2[1],0)  \n",
    "        \n",
    "        arr1 = np.asarray(img1, dtype=\"float32\")\n",
    "        data_x1_list.append(arr1)\n",
    "        \n",
    "        arr2 = np.asarray(img2, dtype=\"float32\")\n",
    "        data_x2_list.append(arr2)\n",
    "        \n",
    "        i += 1\n",
    "        data_y1_list.append(item1[0])\n",
    "        data_y2_list.append(item2[0])\n",
    "\n",
    "data_x1 = np.stack(data_x1_list, axis=0)[:, np.newaxis, :, :]\n",
    "data_y1 = np.stack(data_y1_list, axis=0)\n",
    "data_x2 = np.stack(data_x2_list, axis=0)[:, np.newaxis, :, :]\n",
    "data_y2 = np.stack(data_y2_list, axis=0)\n",
    "\n",
    "# print(data_x1.shape)\n",
    "# print(data_y1.shape)\n",
    "# print(data_x2.shape)\n",
    "# print(data_y2.shape)\n",
    "\n",
    "data_x1 = data_x1 / 255\n",
    "data_x2 = data_x2 / 255\n",
    "data_y1 = np.asarray(data_y1)\n",
    "data_y2 = np.asarray(data_y2)\n",
    "\n",
    "data_x1 = torch.from_numpy(data_x1)\n",
    "data_y1 = torch.from_numpy(data_y1)\n",
    "data_y1 = data_y1.long()\n",
    "\n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(data_x1,\n",
    "                                                    data_y1,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=999,\n",
    "                                                    stratify=data_y1)\n",
    "data_x2 = torch.from_numpy(data_x2)\n",
    "data_y2 = torch.from_numpy(data_y2)\n",
    "data_y2 = data_y2.long()\n",
    "\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(data_x2,\n",
    "                                                    data_y2,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=999,\n",
    "                                                    stratify=data_y2)\n",
    "\n",
    "\n",
    "data1 = Data.TensorDataset(X1_train, X2_train, Y1_train)\n",
    "data2 = Data.TensorDataset(X1_test, X2_test, Y1_test)\n",
    "train_loader = Data.DataLoader(data1, batch_size=24,shuffle=True)\n",
    "valid_loader = Data.DataLoader(data2, batch_size=24)\n",
    "# print(len(data1))\n",
    "# print(len(data2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build the CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN) hyperparameters:\n",
    "1. **Output Channels (out_channels):**\n",
    "    1. Description: This parameter determines the number of filters (or kernels) used in each convolutional layer. Each filter extracts different features from the input image, so increasing the number of output channels generally allows the network to capture more complex features.\n",
    "    2. Effect of Increasing:\n",
    "       1. Increases the computational complexity of the model.\n",
    "       2. Can capture more detailed features from the input, potentially improving model performance on complex tasks.\n",
    "    3. Common Range: Typically starts from 16 and may go up to 128 or more in deeper layers.\n",
    "2. **Kernel Size (kernel_size):**\n",
    "    1. Description: The size of the filter that is applied to the input or the previous layer to produce a feature map. It's usually a square shape.\n",
    "    2. Effect of Increasing:\n",
    "        1. Larger kernels cover more area of the input image, capturing more global features.\n",
    "        2. Can lead to more computational cost and potentially smoother features, but might lose small, detailed features.\n",
    "    3. Common Range: Often 3x3, 5x5, sometimes 7x7.\n",
    "3. **Stride (stride):**\n",
    "    1. Description:The number of pixels the kernel moves across the image or feature map during the convolution operation. A stride of 1 moves the filter one pixel at a time.\n",
    "    2. Effect of Increasing:\n",
    "        1. Increases the downsampling effect, reducing the size of the output feature map faster.\n",
    "        2. Reduces the overlap between receptive fields, potentially losing fine details.\n",
    "    3. Common Range: 1 or 2. A stride of more than 2 is less common but might be used in very deep networks or specific architectures.\n",
    "4. **Padding (padding):**\n",
    "    1. Description:Padding adds zeros around the border of the input image or feature map. This allows the convolutional layer to produce feature maps that are the same size as the input, maintaining the spatial dimensions after convolution.\n",
    "    2. Effect of Increasing:\n",
    "        1. Ensures that the feature map does not shrink after applying the filter, which is especially important in deep networks to keep useful information.\n",
    "        2. Allows the filter to properly operate on the elements at the edge of the input.\n",
    "    3. Common Range: Often set to zero (valid padding) or to a value that makes the output size equal to the input size (same padding, often kernel_size/2 for odd kernels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()   # 继承__init__功能\n",
    "#         ## 第一层卷积\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             # 输入[1,64,64]\n",
    "#             nn.Conv2d(\n",
    "#                 in_channels=1,    # 输入图片的高度\n",
    "#                 out_channels=16,  # 输出图片的高度\n",
    "#                 kernel_size=5,    # 5x5的卷积核，相当于过滤器\n",
    "#                 stride=1,         # 卷积核在图上滑动，每隔一个扫一次\n",
    "#                 padding=2,        # 给图外边补上0\n",
    "#             ),\n",
    "#             # 经过卷积层 输出[16,64,64] 传入池化层\n",
    "#             nn.MaxPool2d(kernel_size=2)   # 经过池化 输出[16,32,32] 传入下一个卷积\n",
    "#         )\n",
    "#         ## 第二层卷积\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(\n",
    "#                 in_channels=16,    # 同上\n",
    "#                 out_channels=32,\n",
    "#                 kernel_size=5,\n",
    "#                 stride=1,\n",
    "#                 padding=2\n",
    "#             ),\n",
    "#             # 经过卷积 输出[32, 32, 32] 传入池化层\n",
    "#             nn.MaxPool2d(kernel_size=2)  # 经过池化 输出[32,16,16] 传入输出层\n",
    "#         )\n",
    "#         ## 第三层卷积\n",
    "#         self.conv3 = nn.Sequential(\n",
    "#             nn.Conv2d(\n",
    "#                 in_channels=32,    # 同上\n",
    "#                 out_channels=64,\n",
    "#                 kernel_size=5,\n",
    "#                 stride=1,\n",
    "#                 padding=2\n",
    "#             ),\n",
    "#             # 经过卷积 输出[64, 16, 16] 传入池化层\n",
    "#             nn.MaxPool2d(kernel_size=2)  # 经过池化 输出[64,8,8] 传入输出层\n",
    "#         )\n",
    "#           ## 第四层卷积\n",
    "#         self.conv4 = nn.Sequential(\n",
    "#             nn.Conv2d(\n",
    "#                 in_channels=64,    # 同上\n",
    "#                 out_channels=64,\n",
    "#                 kernel_size=5,\n",
    "#                 stride=1,\n",
    "#                 padding=2\n",
    "#             ),\n",
    "#             # 经过卷积 输出[64, 8, 8] 传入池化层\n",
    "#             nn.MaxPool2d(kernel_size=2)  # 经过池化 输出[64,4,4] 传入输出层\n",
    "#         )\n",
    "            \n",
    "#         ## 输出层\n",
    "        \n",
    "#         self.output = nn.Sequential(\n",
    "#             nn.Linear(in_features=64*2*2*2, out_features=128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(in_features=128, out_features=3)\n",
    "#         )\n",
    "    \n",
    "\n",
    "#     def forward(self, x1, x2):           # [64×64×1]\n",
    "#         x1 = self.conv1(x1)           # [64×64×16]\n",
    "#         x1 = self.conv2(x1)           # [64×64×16]\n",
    "        \n",
    "#         x2 = self.conv1(x2)           # [64×64×16]\n",
    "#         x2 = self.conv2(x2)           # [64×64×16] \n",
    "           \n",
    "        \n",
    "#         x = torch.cat((x1,x2),3)\n",
    "\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.conv4(x)\n",
    "        \n",
    "#         x = x.view(x.size(0), -1)   # 保留batch, 将后面的乘到一起 [batch, 32*7*7]\n",
    "               \n",
    "#         output = self.output(x)     # 输出[50,10]\n",
    "#         return output\n",
    "\n",
    "# model = CNN()\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, conv_params):\n",
    "        super(CNN, self).__init__()\n",
    "        # Unpacking parameters for each layer\n",
    "        c1_out, c1_kernel, c1_stride, c1_padding = conv_params['conv1']\n",
    "        c2_out, c2_kernel, c2_stride, c2_padding = conv_params['conv2']\n",
    "        c3_out, c3_kernel, c3_stride, c3_padding = conv_params['conv3']\n",
    "        c4_out, c4_kernel, c4_stride, c4_padding = conv_params['conv4']\n",
    "\n",
    "        # Define the layers using the unpacked parameters\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, c1_out, c1_kernel, c1_stride, c1_padding),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(c1_out, c2_out, c2_kernel, c2_stride, c2_padding),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(c2_out, c3_out, c3_kernel, c3_stride, c3_padding),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(c3_out, c4_out, c4_kernel, c4_stride, c4_padding),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(c4_out * 4 * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        \n",
    "        x2 = self.conv1(x2)\n",
    "        x2 = self.conv2(x2)\n",
    "        \n",
    "        x = torch.cat((x1, x2), 3)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "# Function to create a slider with customized width\n",
    "def create_custom_slider(description, value, min, max, step):\n",
    "    return widgets.IntSlider(\n",
    "        value=value,\n",
    "        min=min,\n",
    "        max=max,\n",
    "        step=step,\n",
    "        description=description,\n",
    "        style={'description_width': 'initial'},  # This allows the description to take as much space as it needs\n",
    "        layout=widgets.Layout(width='50%')  # Adjust the width of the slider itself\n",
    "    )\n",
    "\n",
    "# Example usage in a model parameter context\n",
    "def create_param_sliders():\n",
    "    params = {\n",
    "        'conv1': [16, 5, 1, 2],  # example values: out_channels, kernel_size, stride, padding\n",
    "        'conv2': [32, 5, 1, 2],\n",
    "        'conv3': [64, 5, 1, 2],\n",
    "        'conv4': [64, 5, 1, 2]\n",
    "    }\n",
    "    sliders = {}\n",
    "    for layer, param in params.items():\n",
    "        sliders[layer] = [\n",
    "            create_custom_slider(f'{layer} out_channels', param[0], 16, 128, 16),\n",
    "            create_custom_slider(f'{layer} kernel_size', param[1], 3, 7, 1),\n",
    "            create_custom_slider(f'{layer} stride', param[2], 1, 3, 1),\n",
    "            create_custom_slider(f'{layer} padding', param[3], 0, 4, 1)\n",
    "        ]\n",
    "    return sliders\n",
    "\n",
    "sliders = create_param_sliders()\n",
    "ui = widgets.VBox([widgets.VBox(s) for s in sliders.values()])\n",
    "display(ui)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    conv_params = {k: [s.value for s in v] for k, v in sliders.items()}\n",
    "    global model\n",
    "    model = CNN(conv_params)\n",
    "    print(model)\n",
    "    \n",
    "button = widgets.Button(description=\"Update Model\")\n",
    "button.on_click(on_button_clicked)\n",
    "display(button)\n",
    "\n",
    "def confusion_matrix(labels, preds, conf_matrix):\n",
    "    for p, t in zip(labels, preds):\n",
    "        conf_matrix[p, t] += 1\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=[(1, 32, 32), (1, 32, 32)])  # Specify the input size of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# writer = SummaryWriter('runs/model_visualization')\n",
    "# dummy_input1 = torch.randn(1, 1, 32, 32)\n",
    "# dummy_input2 = torch.randn(1, 1, 32, 32)\n",
    "# writer.add_graph(model, (dummy_input1, dummy_input2))\n",
    "# writer.close()\n",
    "# torch.onnx.export(model, (dummy_input1, dummy_input2), \"model.onnx\")\n",
    "# !netron model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "loss_f = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "epochs = 30\n",
    "Train_epoch, Test_epoch, Train_accuracy, Test_accuracy, Loss = [], [], [], [], []\n",
    "Train_time, Test_time = [], []\n",
    "time0 = time.time()\n",
    "Predict_label, True_label = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time1 = time.time()\n",
    "    Train_epoch.append(epoch + 1)\n",
    "    running_loss, running_correct = 0, 0\n",
    "    print(\"Epoch {}/{}\".format(epoch + 1, epochs))\n",
    "    print(\"-\" * 10)\n",
    "\n",
    "    # Wrap train_loader with tqdm for a progress bar\n",
    "    for data in tqdm(train_loader, desc=\"Training\"):\n",
    "        X1_train, X2_train, Y_train = data\n",
    "        # Assume model and data are on the same device, add .to(device) if needed\n",
    "        y_pred = model(X1_train, X2_train)\n",
    "        loss = loss_f(y_pred, Y_train)\n",
    "        pred = torch.max(y_pred, 1)[1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_correct += torch.sum(pred == Y_train).item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = running_correct / len(train_loader.dataset) *100\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train ACC: {train_acc:.4f}%\")\n",
    "\n",
    "    Train_time.append(time.time() - time1)\n",
    "    Loss.append(train_loss)\n",
    "    Train_accuracy.append(train_acc * 100)\n",
    "\n",
    "    # Validation loop with tqdm\n",
    "    test_loss, test_correct = 0, 0\n",
    "    conf_matrix = torch.zeros(3, 3)\n",
    "    for data in tqdm(valid_loader, desc=\"Validation\"):\n",
    "        X1_test, X2_test, Y_test = data\n",
    "        outputs = model(X1_test, X2_test)\n",
    "        pred = torch.max(outputs, 1)[1]\n",
    "        loss = loss_f(outputs, Y_test)\n",
    "        \n",
    "        if epoch == epochs-1:\n",
    "            Predict_label.append(pred.numpy())\n",
    "            True_label.append(Y_test.numpy())\n",
    "            conf_matrix = confusion_matrix(Y_test, pred, conf_matrix)\n",
    "            \n",
    "        test_loss += loss.item()\n",
    "        test_correct += torch.sum(pred == Y_test).item()\n",
    "\n",
    "    test_loss /= len(valid_loader.dataset)\n",
    "    test_accuracy = test_correct / len(valid_loader.dataset) *100\n",
    "    print(f\"Valid Loss: {test_loss:.4f}, Valid ACC: {test_accuracy:.4f}%\")\n",
    "\n",
    "    Test_accuracy.append(test_accuracy)\n",
    "    Test_time.append(time.time() - time1)\n",
    "\n",
    "# Save results to files and print the confusion matrix\n",
    "np.savetxt('CNN1-train_time_two_sensor_feature_micpho1_32.txt', Train_time, fmt=\"%.4f\")\n",
    "np.savetxt('CNN1-test_time_two_sensor_feature_micpho1_32.txt', Test_time, fmt=\"%.4f\")\n",
    "save_fn = 'CNN1_two_sensor_feature_micpho1_32.mat'\n",
    "sio.savemat(save_fn, {'train_epoch': Train_epoch, 'train_accuracy': Train_accuracy,\n",
    "                              'test_epoch': Test_epoch, 'test_accuracy': Test_accuracy,\n",
    "                              'train_loss': Loss, \n",
    "                              'predict_label': Predict_label, \n",
    "                              'true_label': True_label})\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "conf_matrix=conf_matrix.numpy()\n",
    "conf_matrix=conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "conf_matrix = np.around(conf_matrix, decimals=4)\n",
    "\n",
    "plt.rc('font',family='Times New Roman',size=16)\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "df_cm = pd.DataFrame(conf_matrix,\n",
    "                     index = [\"Low quality\",\"Medium quality\",\"High quality\"],\n",
    "                     columns = [\"Low quality\",\"Medium quality\",\"High quality\"])\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 24},cmap=\"Blues\",fmt='.4f')\n",
    "plt.gca().set_title('Confusion matrix',fontsize=24)\n",
    "plt.gca().set_xlabel('Predict label',fontsize=24)\n",
    "plt.gca().set_ylabel('True label',fontsize=24)\n",
    "#plt.gca().xaxis.set_ticks_position('none') \n",
    "#plt.gca().yaxis.set_ticks_position('none')\n",
    "plt.gca().set_yticklabels(plt.gca().get_yticklabels(), rotation=0)\n",
    "plt.grid(True, which='minor', linewidth=0.8 , linestyle='-')\n",
    "plt.subplots_adjust(top = 0.99, bottom = 0.12, right = 1.02, left = 0.12, hspace = 0, wspace = 0) #调整图像边缘\n",
    "plt.margins(0,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample output\n",
    "\n",
    "<img src=\"./figures/confusion_matrix.png\" alt=\"Sample Image\" width=\"50%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
