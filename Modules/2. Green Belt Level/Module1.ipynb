{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b304761e",
   "metadata": {
    "tags": [
     "auto-generated-toc"
    ]
   },
   "source": [
    "## Table of Contents\n",
    "- [G1 Data](#G1-Data)\n",
    "  - [1.1 Handling Structured Data in Digital Engineering](#1.1-Handling-Structured-Data-in-Digital-Engineering)\n",
    "  - [1.2 Data Visualization](#1.2-Data-Visualization)\n",
    "  - [1.3 Data Preprocessing](#1.3-Data-Preprocessing)\n",
    "- [üè† Home](../../welcomePage.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da8e87",
   "metadata": {},
   "source": [
    "# G1 Data\n",
    "## 1.1 Handling Structured Data in Digital Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d63ec2",
   "metadata": {},
   "source": [
    "Ensure you are familiar with the foundational concepts from the Yellow Belt Data Module before proceeding. In this module, we will work with a manufacturing dataset to develop practical skills in three key areas: data loading and exploration, data cleaning, and outlier detection and handling. We will use Python for examples, but non-coders will still be able to follow along.\n",
    "\n",
    "- **Data Loading and Exploration:** Efficiently importing and examining structured datasets to uncover patterns and initial insights.\n",
    "- **Data Cleaning:** Addressing missing values, ensuring consistency, and preparing the dataset for robust analysis.\n",
    "- **Outlier Detection and Handling:** Identifying anomalies and making data-driven decisions on their treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf88528",
   "metadata": {},
   "source": [
    "In this example dataset, we aim to explore and understand the factors influencing the manufacturing process. By examining various variables such as temperature, rotational speed, torque, and tool wear, along with the outcome (target) and failure types, we can identify patterns and relationships that may contribute to product failures.\n",
    "\n",
    "### <font color = '#646464'> 1.1.1 Loading the Dataset </font>\n",
    "\n",
    "In Python, you need to import libraries, which are like tools that help you perform specific tasks. To start, we will import the `pandas` library, which is essential for working with and analyzing data. Next, we load our dataset from a CSV file using the `pandas.read_csv()` function. This step requires specifying the file's location, which in this case is within the `Module 1 Content` folder.\n",
    "\n",
    "If you want to see the code for this step, you can click on the \"view\" symbol to reveal it. Clicking it again will hide the code. Each cell in this module has this option.\n",
    "\n",
    "Once the dataset is loaded, we use `df.head()` to preview the first few rows. This gives us a quick look at the data, showing the column types and values, so we can confirm that the data has loaded correctly and get an initial understanding of its structure.\n",
    "\n",
    "<center>\n",
    "  <img src=\"Module 1 Content/extruderDetails.png\" alt=\"Quantitative vs Qualitative Data\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "**Data Description**\n",
    "\n",
    "1. **UDI**: Unique identifier for each record.\n",
    "2. **Product ID**: Identifier for the product being manufactured.\n",
    "3. **Type**: The type or category of the product.\n",
    "4. **Air temperature [K]**: The air temperature in Kelvin during the manufacturing process.\n",
    "5. **Process temperature [K]**: The process temperature in Kelvin during the manufacturing process.\n",
    "6. **Rotational speed [rpm]**: The rotational speed of the machinery in revolutions per minute.\n",
    "7. **Torque [Nm]**: The torque exerted by the machinery in Newton meters.\n",
    "8. **Tool wear [min]**: The tool wear time in minutes.\n",
    "9. **Target**: The target variable indicating the outcome or result of the manufacturing process.\n",
    "10. **Failure Type**: The type of failure observed, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3248f92",
   "metadata": {
    "has_explanation": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Module 1 Content/predictive_maintenance.csv')\n",
    "\n",
    "# View Dataset\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eea831",
   "metadata": {},
   "source": [
    "### <font color = '#646464'> 1.1.2 Inspecting Challenges in the Dataset* </font>\n",
    "Once the data is loaded, it's crucial to inspect it for any common issues that might affect the analysis. \n",
    "As we discussed in the Yellow Belt Data Module, this includes checking for missing values, outliers, and incorrect data types. \n",
    "In practice, we use `df.info()` to get a concise summary of the dataframe, including the number of non-null entries and data type of each column. If we have less values in a variable than the total entries, it means some were missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88acf416",
   "metadata": {
    "has_explanation": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Inspect data for common issues\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f2d125",
   "metadata": {},
   "source": [
    "The dataset inspection using `df.info()` reveals missing values in several operational parameters such as 'Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', and 'Tool wear [min]'. These features do not have values for each of the 10000 samples. This is not uncommon in manufacturing datasets, where such issues can occur due to several factors:\n",
    "\n",
    "- **Sensor Malfunctions:** Critical sensor failures or maintenance lapses can prevent accurate data logging.\n",
    "- **Data Transmission Errors:** Issues in data transmission can lead to incomplete records, especially in environments with electronic interferences.\n",
    "- **Operational Disruptions:** Adjustments in production processes or machine downtimes often result in data capture gaps.\n",
    "\n",
    "**Methods to Handle Missing Data:**\n",
    "\n",
    "- **Imputation:** Filling in missing data with estimated values, like the average or median of the column.\n",
    "  \n",
    "- **Removal:** Removing rows with missing data when the amount of missing information is small and doesn't affect the analysis.\n",
    "\n",
    "- **Flagging:** Marking missing values so they can be identified during analysis without changing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e627a1",
   "metadata": {},
   "source": [
    "### <font color = '#646464'> 1.1.3 Generating Summary Statistics </font>\n",
    "\n",
    "In data analysis, generating summary statistics is a key step to understanding the dataset's central tendency, dispersion, and overall distribution. Summary statistics offer a concise way to summarize the essential characteristics of numerical data, and they are foundational in exploratory data analysis (EDA).\n",
    "\n",
    "- **Central Tendency**: This refers to the measures that represent the center or typical value of a dataset. The most common measures are:\n",
    "  - **Mean**: The average value of the data, calculated as the sum of all data points divided by the number of data points.\n",
    "  - **Median**: The middle value when the data is sorted in ascending or descending order. It is less sensitive to outliers than the mean.\n",
    "  - **Mode**: The most frequently occurring value in the dataset.\n",
    "\n",
    "<center>\n",
    "  <img src=\"Module 1 Content/meanmedian.png\" alt=\"Quantitative vs Qualitative Data\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "- **Dispersion**: This refers to the spread or variability in the dataset. Common measures include:\n",
    "  - **Standard Deviation**: This measures how much individual data points differ from the mean. A higher standard deviation indicates that the data is more spread out.\n",
    "  - **Variance**: The square of the standard deviation, representing the average of the squared differences from the mean.\n",
    "  - **Range**: The difference between the maximum and minimum values in the dataset.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Shape of Distribution**: This refers to how the data is distributed across values. Commonly assessed through:\n",
    "  - **Skewness**: A measure of the asymmetry of the data distribution. Positive skew indicates that the data‚Äôs tail is skewed to the right, while negative skew indicates a left skew.\n",
    "  - **Kurtosis**: A measure of the \"tailedness\" of the data distribution, showing how outliers or extreme values are distributed.\n",
    "\n",
    "To further understand the dataset, we generate summary statistics which provide insights into the central tendency,\n",
    "dispersion, and shape of the dataset‚Äôs distribution. We use `df.describe()` which includes mean, standard deviation, minimum, and maximum values for numerical columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc2b58",
   "metadata": {
    "has_explanation": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bfb6fd",
   "metadata": {},
   "source": [
    "The dataset's descriptive statistics reveal key insights into the manufacturing process:\n",
    "\n",
    "- **Temperature Stability**: Look at the values of air temperature (ranging from 295.3 K to 304.5 K) and process temperature (ranging from 305.7 K to 313.8 K). Since the standard deviation for both is low (2.0 K for air temperature and 1.5 K for process temperature), it indicates that temperature variability is minimal. This shows that the environment is well-controlled, which is essential for maintaining consistency and quality in the manufacturing process.\n",
    "  \n",
    "- **Rotational Speed Variance**: The range of rotational speeds (from 1168 rpm to 2886 rpm) and a relatively higher standard deviation (179.88) suggest that there is a considerable amount of variability in the rotational speeds. This could indicate either operational adjustments or different machine settings used during production.\n",
    "\n",
    "- **Torque Variability**: Torque values range from 3.8 Nm to 76.6 Nm, with a standard deviation of 9.97. This significant variability indicates that torque fluctuates considerably, likely due to varying operational demands or changing material conditions during production.\n",
    "\n",
    "- **Tool Wear Range**: Tool wear shows a wide range, from 0 to 253 minutes, with a high standard deviation (63.66). This suggests that the amount of tool wear varies significantly, possibly due to differences in machine usage intensity or maintenance practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632bc451",
   "metadata": {},
   "source": [
    "## 1.2 Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8445f16b",
   "metadata": {},
   "source": [
    "### <font color = '#646464'> 1.2.1 Input Feature Data Visualization </font>\n",
    "Visualizing data helps in identifying relationships between variables, patterns, and potential outliers.\n",
    "\n",
    "#### Importance of Data Visualization\n",
    "\n",
    "- **Identifying Relationships**: Scatter plots and other visualizations can help reveal relationships between variables, such as linear or non-linear correlations. For example, if two variables show a positive or negative trend when plotted, this can indicate a potential relationship.\n",
    "  \n",
    "- **Pattern Recognition**: Visualizations allow us to recognize patterns, clusters, or trends that might not be immediately apparent from raw data alone. For instance, scatter plots can highlight clusters of data points or identify outliers, which may indicate anomalies or errors.\n",
    "\n",
    "- **Outlier Detection**: By plotting data points, outliers (values that fall outside the general distribution) become immediately visible. These outliers may indicate measurement errors, rare events, or special cases that need further investigation.\n",
    "\n",
    "Here, we'll plot `Air temperature [K]` against `Process temperature [K]` to see how these temperatures relate in the manufacturing process. We will use `matplotlib` for plotting, which provides a straightforward way to create scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21517e0a",
   "metadata": {
    "has_explanation": false
   },
   "outputs": [],
   "source": [
    "print(\"‚úÖ Code Running\")\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "# Scatter plot\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.scatter(df['Air temperature [K]'].iloc[::20], df['Process temperature [K]'].iloc[::20], s = 5)\n",
    "plt.title('Air Temperature vs Process Temperature')\n",
    "plt.xlabel('Air Temperature [K]')\n",
    "plt.ylabel('Process Temperature [K]')\n",
    "clear_output(wait=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3cd61f",
   "metadata": {},
   "source": [
    "The plot show direct positive correlation between the Air and Process Temperatures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f168c3",
   "metadata": {},
   "source": [
    "A histogram is a graphical representation of the distribution of a dataset, showing the frequency of data points within specific ranges. It allows us to visually assess the shape of the data, such as whether it is symmetric, skewed, or multimodal. By observing the spread and concentration of data within the bins, we can gauge the variability and identify any outliers. In the context of the air temperature dataset, the histogram helps to understand how temperature values are distributed, whether there is significant variation, and if the data follows a particular trend or pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657c683",
   "metadata": {
    "has_explanation": false
   },
   "outputs": [],
   "source": [
    "print(\"‚úÖ Code Running\")\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "# Plotting histogram for Air Temperature [K]\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.hist(df['Air temperature [K]'], bins=20, edgecolor='black')\n",
    "plt.title('Air Temperature Distribution')\n",
    "plt.xlabel('Air Temperature [K]')\n",
    "plt.ylabel('Frequency')\n",
    "clear_output(wait=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c4d67",
   "metadata": {},
   "source": [
    "A box plot provides a summary of a dataset's distribution, highlighting key statistics such as the median, quartiles, and potential outliers. It visually represents the spread of the data and helps to identify any extreme values or outliers. The box shows the interquartile range (IQR), where the middle 50% of the data lies, while the line inside the box represents the median value. Any data points outside the \"whiskers\" (the range within 1.5 times the IQR) are considered outliers. In the case of the rotational speed dataset, the box plot reveals the central tendency of the data, the spread, and any extreme values, allowing for a clearer understanding of the variability in machine speed.\n",
    "\n",
    "<center>\n",
    "  <img src=\"Module 1 Content/boxplot.png\" alt=\"Quantitative vs Qualitative Data\" width=\"300\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc7f3c",
   "metadata": {
    "has_explanation": false
   },
   "outputs": [],
   "source": [
    "print(\"‚úÖ Code Running\")\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "# Box plot for Air Temperature [K] to detect outliers\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.boxplot(df['Rotational speed [rpm]'].dropna(), \n",
    "            flierprops=dict(marker='o', color='red', markersize=3), \n",
    "            whis=4.5)  # Change whis value (default is 1.5, try something smaller or larger)\n",
    "plt.title('Rotational Speed [rpm] Distribution (Box Plot)')  # Title\n",
    "plt.ylabel('Rotational Speed [rpm]')  # Y-axis label\n",
    "clear_output(wait=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eebb6b",
   "metadata": {},
   "source": [
    "### <font color = '#646464'> 1.2.2 Corresponding Output: Visualizing Failure Types </font>\n",
    "\n",
    "Understanding the distribution of failure types is crucial for identifying common issues and prioritizing maintenance efforts. A bar plot is an effective way to visualize the counts of different failure types in the dataset. This visualization helps in quickly identifying the most frequent types of failures and making informed decisions for improving the manufacturing process.\n",
    "\n",
    "We will use `matplotlib` to create the bar plot, which provides a clear representation of the failure type counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7a67e",
   "metadata": {
    "has_explanation": false
   },
   "outputs": [],
   "source": [
    "print(\"‚úÖ Code Running\")\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set global font properties for all matplotlib plots\n",
    "plt.rcParams['font.size'] = 8  # Set a default smaller font size\n",
    "\n",
    "\n",
    "# Calculate the counts of each failure type\n",
    "failure_counts = df['Failure Type'].value_counts()\n",
    "\n",
    "# Calculate the percentage for each failure type\n",
    "failure_percentages = (failure_counts / failure_counts.sum()) * 100\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(failure_counts.index, failure_counts, color='skyblue')\n",
    "\n",
    "# Add percentage labels on top of the bars\n",
    "for bar, percentage in zip(bars, failure_percentages):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{percentage:.1f}%', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.title('Counts of Different Failure Types')\n",
    "# plt.xlabel('Failure Type')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y')\n",
    "clear_output(wait=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf1533",
   "metadata": {},
   "source": [
    "## 1.3 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66914475",
   "metadata": {},
   "source": [
    "### <font color = '#646464'> 1.3.1 Handling Missing Data </font>\n",
    "\n",
    "Handling missing data is crucial in ensuring the quality and reliability of analyses. We will explore two common methods:\n",
    "\n",
    "1. **Dropping missing data**: This approach involves removing rows or columns that contain missing values, which is straightforward but may result in loss of valuable data if not used carefully.\n",
    "2. **Filling missing data**: This method involves replacing missing values with statistically relevant figures (mean, median, or mode) or using methods like forward/backward filling, which are particularly useful in time-series data.\n",
    "\n",
    "Both techniques help in maintaining the integrity of the dataset for robust analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894afff",
   "metadata": {},
   "source": [
    "Option 1: Delete Rows with Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51651f",
   "metadata": {
    "has_explanation": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Option 1: Delete rows with missing data\n",
    "\n",
    "# Drop rows with any missing values\n",
    "drop_df = df.dropna()\n",
    "\n",
    "# We inspect the data again\n",
    "drop_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2604c3b7",
   "metadata": {},
   "source": [
    "Option 2: Fill Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7817a",
   "metadata": {},
   "source": [
    "1. **Fill Numerical Data with Mean**: Missing values in numerical columns are replaced with the average (mean) of the column. This ensures the numbers remain consistent and balanced.\n",
    "\n",
    "2. **Fill Categorical Data with Mode**: Missing values in categorical columns are replaced with the most frequent value (mode). This keeps the categories representative of the data.\n",
    "\n",
    "#### Steps\n",
    "- Separate the numerical and categorical columns.\n",
    "- Fill numerical columns with their mean values.\n",
    "- Fill categorical columns with their mode values.\n",
    "- Combine the processed columns back into a single dataset.\n",
    "\n",
    "This approach ensures the dataset is complete and ready for analysis while maintaining its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67e2bc",
   "metadata": {
    "has_explanation": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fill numerical columns with mean\n",
    "df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
    "df_filled_numeric = df_numeric.fillna(df_numeric.mean())\n",
    "\n",
    "# Fill categorical columns with mode\n",
    "df_categorical = df.select_dtypes(include=['object'])\n",
    "df_filled_categorical = df_categorical.apply(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "# Combine the filled numerical and categorical DataFrames\n",
    "df_filled = pd.concat([df_filled_numeric, df_filled_categorical], axis=1)\n",
    "\n",
    "df_filled.info()\n",
    "data = df_filled.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffda23c",
   "metadata": {},
   "source": [
    "### <font color = '#646464'> 1.3.2 Encoding Techniques for Categorical Data </font>\n",
    "\n",
    "Categorical data often needs to be converted into a numerical format for analysis and machine learning tasks. Many algorithms require numerical input, and raw categorical data may not provide sufficient insights or be suitable for mathematical operations. Below are several encoding techniques used to convert categorical data into numerical formats:\n",
    "\n",
    "#### <font color = '#646464'> 1.3.2.1 One-Hot Encoding </font>\n",
    "One-hot encoding represents each category as a binary column. If a category is present in a record, the corresponding column is marked as `1`, otherwise `0`. This method is useful for categorical variables with no ordinal relationship.\n",
    "\n",
    "##### **Before One-Hot Encoding**\n",
    "\n",
    "| Item             | Category      |\n",
    "|------------------|---------------|\n",
    "| Laptop           | Electronics   |\n",
    "| Chair            | Furniture     |\n",
    "| T-shirt          | Clothing      |\n",
    "| Fridge           | Appliances    |\n",
    "| Washing Machine  | Appliances    |\n",
    "| Desk             | Furniture     |\n",
    "| Jacket           | Clothing      |\n",
    "| Microwave        | Appliances    |\n",
    "| Sofa             | Furniture     |\n",
    "| Shoes            | Clothing      |\n",
    "\n",
    "##### **After One-Hot Encoding**\n",
    "\n",
    "| Item             | Electronics | Furniture | Clothing | Appliances |\n",
    "|------------------|-------------|-----------|----------|------------|\n",
    "| Laptop           | 1           | 0         | 0        | 0          |\n",
    "| Chair            | 0           | 1         | 0        | 0          |\n",
    "| T-shirt          | 0           | 0         | 1        | 0          |\n",
    "| Fridge           | 0           | 0         | 0        | 1          |\n",
    "| Washing Machine  | 0           | 0         | 0        | 1          |\n",
    "| Desk             | 0           | 1         | 0        | 0          |\n",
    "| Jacket           | 0           | 0         | 1        | 0          |\n",
    "| Microwave        | 0           | 0         | 0        | 1          |\n",
    "| Sofa             | 0           | 1         | 0        | 0          |\n",
    "| Shoes            | 0           | 0         | 1        | 0          |\n",
    "\n",
    "\n",
    "#### <font color = '#646464'> 1.3.2.2 Label Encoding </font>\n",
    "Label encoding assigns a unique numerical value to each category. While simple and efficient for ordinal data, it may introduce unintended ordinal relationships in nominal data.\n",
    "\n",
    "##### **Before Label Encoding**\n",
    "\n",
    "| Item             | Category      |\n",
    "|------------------|---------------|\n",
    "| Laptop           | Electronics   |\n",
    "| Chair            | Furniture     |\n",
    "| T-shirt          | Clothing      |\n",
    "| Fridge           | Appliances    |\n",
    "| Washing Machine  | Appliances    |\n",
    "| Desk             | Furniture     |\n",
    "| Jacket           | Clothing      |\n",
    "| Microwave        | Appliances    |\n",
    "| Sofa             | Furniture     |\n",
    "| Shoes            | Clothing      |\n",
    "\n",
    "##### **After Label Encoding**\n",
    "\n",
    "| Item             | Category      | Category (Encoded) |\n",
    "|------------------|---------------|--------------------|\n",
    "| Laptop           | Electronics   | 0                  |\n",
    "| Chair            | Furniture     | 1                  |\n",
    "| T-shirt          | Clothing      | 2                  |\n",
    "| Fridge           | Appliances    | 3                  |\n",
    "| Washing Machine  | Appliances    | 3                  |\n",
    "| Desk             | Furniture     | 1                  |\n",
    "| Jacket           | Clothing      | 2                  |\n",
    "| Microwave        | Appliances    | 3                  |\n",
    "| Sofa             | Furniture     | 1                  |\n",
    "| Shoes            | Clothing      | 2                  |\n",
    "\n",
    "#### <font color = '#646464'> 1.3.2.3 Frequency Encoding </font>\n",
    "Frequency encoding replaces categories with their frequency of occurrence in the dataset. This method is useful when the frequency of categories carries important information.\n",
    "\n",
    "##### **Before Frequency Encoding**  \n",
    "\n",
    "| Failure Type              |\n",
    "|---------------------------|\n",
    "| None                      |\n",
    "| Heat Dissipation Failure  |\n",
    "| Power Failure             |\n",
    "| None                      |\n",
    "| Power Failure             |\n",
    "| Heat Dissipation Failure  |\n",
    "\n",
    "##### **After Frequency Encoding**  \n",
    "\n",
    "| Failure Type              | Frequency Encoding |\n",
    "|---------------------------|--------------------|\n",
    "| None                      | 0.50               |\n",
    "| Heat Dissipation Failure  | 0.30               |\n",
    "| Power Failure             | 0.20               |\n",
    "| None                      | 0.50               |\n",
    "| Power Failure             | 0.20               |\n",
    "| Heat Dissipation Failure  | 0.30               |\n",
    "\n",
    "\n",
    "\n",
    "#### Choosing the Right Technique\n",
    "Each encoding technique has its own advantages and is suitable for different scenarios. It is important to select the appropriate method based on:\n",
    "- The nature of the categorical data (nominal or ordinal).\n",
    "- The requirements of the analysis or machine learning model.\n",
    "\n",
    "By effectively converting categorical data into numerical formats, you can perform more meaningful analyses and enhance the performance of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a0009d",
   "metadata": {},
   "source": [
    " **Back to Example**, we applied encoding to the categorical data in the `Failure Type` and `Type` columns:\n",
    "\n",
    "1. **Failure Type**: We used **Label Encoding** to convert the categorical values into numerical format. Specifically, the new column `No Failure Encoded` assigns:\n",
    "   - `1` when there is \"No Failure.\"\n",
    "   - `0` for any type of failure.\n",
    "\n",
    "2. **Type**: We used **One-Hot Encoding** to create binary columns for each category in the `Type` column, representing the presence (`1`) or absence (`0`) of each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a35af6",
   "metadata": {
    "has_explanation": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"‚úÖ Code Running\")\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Label Encoding for Failure Type\n",
    "label_encoder = LabelEncoder()\n",
    "data['No Failure Encoded'] = label_encoder.fit_transform(data['Failure Type'])\n",
    "\n",
    "# One-Hot Encoding for Type\n",
    "data_encoded = pd.get_dummies(data, columns=['Type'], prefix='Type')\n",
    "\n",
    "# Display the updated DataFrame\n",
    "clear_output(wait=True)\n",
    "data_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20a0dd6",
   "metadata": {},
   "source": [
    "To remove unnecessary columns such as `UDI` (a unique identifier) and `Failure Type` (already encoded as `No Failure Encoded`), we can use the `drop` function from pandas. Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a127f45",
   "metadata": {
    "has_explanation": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "columns_to_drop = ['UDI', 'Failure Type']\n",
    "data_encoded_cleaned = data_encoded.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "data_encoded_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496bfd3c",
   "metadata": {
    "tags": [
     "Bottom Navigation"
    ]
   },
   "source": [
    "### <center>[üè† Home](../../welcomePage.ipynb)‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ[Module 2 ‚ñ∂Ô∏é](Module2.ipynb)</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
